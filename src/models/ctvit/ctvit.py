"""
CTViT: 3D Vision Transformer for CT Volume Processing
åŸºäºæ—¶ç©ºåˆ†ç¦»æ³¨æ„åŠ›æœºåˆ¶çš„VQ-VAEè‡ªç¼–ç å™¨

Architecture Flow:
    Input (B, C, T, H, W) - CT Volumeæ•°æ®
    â†“
    Patch Embedding - å°†3D volumeåˆ‡åˆ†ä¸ºpatches
    â†“
    Spatial Encoder - å¯¹æ¯ä¸ªæ—¶é—´å¸§åšç©ºé—´æ³¨æ„åŠ›
    â†“
    Temporal Encoder - å¯¹åŒä¸€ç©ºé—´ä½ç½®åšæ—¶é—´æ³¨æ„åŠ›
    â†“
    Vector Quantization (VQ) - ç¦»æ•£åŒ–ç¼–ç 
    â†“
    Temporal Decoder - æ—¶é—´ç»´åº¦è§£ç 
    â†“
    Spatial Decoder - ç©ºé—´ç»´åº¦è§£ç 
    â†“
    Pixel Reconstruction - é‡å»ºä¸ºåŸå§‹å°ºå¯¸
    â†“
    Output (B, C, T, H, W) - é‡å»ºçš„volume

Key Features:
    1. Factorized Spatial-Temporal Attention (æ—¶ç©ºåˆ†ç¦»æ³¨æ„åŠ›)
    2. Vector Quantization for discrete representation (VQç¦»æ•£åŒ–è¡¨ç¤º)
    3. Continuous Position Bias (è¿ç»­ä½ç½®åç½®)
    4. PEG Position Encoding (ä½ç½®ç¼–ç ç”Ÿæˆå™¨)

ğŸ”§ [æ•´ä½“æ¶æ„ç°ä»£åŒ–æ”¹é€ æ–¹å‘]:
1. ä½¿ç”¨Flash AttentionåŠ é€Ÿæ³¨æ„åŠ›è®¡ç®—
2. å¼•å…¥Grouped-Query Attentionå‡å°‘KV cache
3. æ›¿æ¢ä¸ºæ›´é«˜æ•ˆçš„ä½ç½®ç¼–ç  (å¦‚RoPE)
4. è€ƒè™‘ä½¿ç”¨æ··åˆä¸“å®¶(MOE)å¢åŠ æ¨¡å‹å®¹é‡
"""

import copy
from pathlib import Path
from typing import Union, Tuple, Optional

import torch
import torch.nn.functional as F
from torch import nn
from einops import rearrange, repeat, pack, unpack
from einops.layers.torch import Rearrange

from vector_quantize_pytorch import VectorQuantize

from .layers import exists, pair
from .attention import Transformer, ContinuousPositionBias


# ============================================================================
# CTViT Main Model
# ============================================================================

class CTViT(nn.Module):
    """
    CTViT: 3D Vision Transformer for CT Volumes

    åŸºäºæ—¶ç©ºåˆ†ç¦»æ³¨æ„åŠ›æœºåˆ¶çš„VQ-VAEæ¨¡å‹ï¼Œç”¨äºCTä½“ç§¯æ•°æ®çš„ç¼–ç å’Œé‡å»º

    Args:
        dim: Transformeréšè—ç»´åº¦ (ä¾‹å¦‚: 512)
        codebook_size: VQç æœ¬å¤§å° (ä¾‹å¦‚: 8192)
        image_size: å›¾åƒå°ºå¯¸ H, W (ä¾‹å¦‚: 480)
        patch_size: Patchå¤§å° (ä¾‹å¦‚: 20, åˆ™æ¯ä¸ªpatchä¸º20x20)
        temporal_patch_size: æ—¶é—´ç»´åº¦patchå¤§å° (ä¾‹å¦‚: 10)
        spatial_depth: ç©ºé—´Transformerå±‚æ•° (ä¾‹å¦‚: 4)
        temporal_depth: æ—¶é—´Transformerå±‚æ•° (ä¾‹å¦‚: 4)
        dim_head: æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„ç»´åº¦ (é»˜è®¤: 64)
        heads: æ³¨æ„åŠ›å¤´æ•° (é»˜è®¤: 8)
        channels: è¾“å…¥é€šé“æ•° (CTé€šå¸¸ä¸º1)
        attn_dropout: æ³¨æ„åŠ›å±‚dropout (é»˜è®¤: 0.)
        ff_dropout: FeedForwardå±‚dropout (é»˜è®¤: 0.)

    Input Shape:
        (B, C, T, H, W) - Batch, Channels, Time, Height, Width

    Output Modes:
        1. é»˜è®¤: (recon_loss, commit_loss, recon_video)
        2. return_recons_only=True: recon_video
        3. return_only_codebook_ids=True: indices
        4. return_encoded_tokens=True: tokens
    """

    def __init__(
        self,
        *,
        dim: int,
        codebook_size: int,
        image_size: int,
        patch_size: int,
        temporal_patch_size: int,
        spatial_depth: int,
        temporal_depth: int,
        dim_head: int = 64,
        heads: int = 8,
        channels: int = 1,
        attn_dropout: float = 0.,
        ff_dropout: float = 0.
    ):
        """
        åˆå§‹åŒ–CTViTæ¨¡å‹

        Einstein Notation:
            b - batch
            c - channels
            t - time (temporal dimension)
            h, w - height, width (spatial dimensions)
            d - feature dimension
            p1, p2 - patch height, patch width
            pt - temporal patch size
        """
        super().__init__()

        # ===== åŸºæœ¬é…ç½® =====
        self.image_size = pair(image_size)  # (H, W)
        self.patch_size = pair(patch_size)  # (pH, pW)
        patch_height, patch_width = self.patch_size

        self.temporal_patch_size = temporal_patch_size

        # æ£€æŸ¥å°ºå¯¸æ˜¯å¦èƒ½è¢«patch sizeæ•´é™¤
        image_height, image_width = self.image_size
        assert (image_height % patch_height) == 0 and (image_width % patch_width) == 0, \
            f"Image size {self.image_size} must be divisible by patch size {self.patch_size}"

        # ===== ä½ç½®ç¼–ç  =====
        # ç©ºé—´ç»´åº¦çš„è¿ç»­ä½ç½®åç½® (ç”¨äºSpatial Transformer)
        self.spatial_rel_pos_bias = ContinuousPositionBias(dim=dim, heads=heads)

        # ===== Patch Embedding =====
        # å°†3D volumeåˆ‡åˆ†ä¸ºpatcheså¹¶æ˜ å°„åˆ°embeddingç©ºé—´
        # Input:  (B, C, T, H, W)
        # Output: (B, T', H', W', D) å…¶ä¸­ T'=T/pt, H'=H/pH, W'=W/pW
        self.to_patch_emb = nn.Sequential(
            # Rearrange: åˆ‡åˆ†patches
            # (B, C, T, H, W) -> (B, T/pt, H/pH, W/pW, C*pt*pH*pW)
            Rearrange(
                'b c (t pt) (h p1) (w p2) -> b t h w (c pt p1 p2)',
                p1=patch_height, p2=patch_width, pt=temporal_patch_size
            ),
            # å½’ä¸€åŒ–
            nn.LayerNorm(channels * patch_width * patch_height * temporal_patch_size),
            # çº¿æ€§æŠ•å½±åˆ°éšè—ç»´åº¦
            nn.Linear(channels * patch_width * patch_height * temporal_patch_size, dim),
            # å†æ¬¡å½’ä¸€åŒ–
            nn.LayerNorm(dim)
        )

        # ===== Transformeré…ç½® =====
        transformer_kwargs = dict(
            dim=dim,
            dim_head=dim_head,
            heads=heads,
            attn_dropout=attn_dropout,
            ff_dropout=ff_dropout,
            peg=True,        # ä½¿ç”¨PEGä½ç½®ç¼–ç 
            peg_causal=True, # æ—¶é—´ç»´åº¦ä½¿ç”¨å› æœpadding
        )

        # ===== ç¼–ç å™¨ (Encoder) =====
        # 1. ç©ºé—´ç¼–ç å™¨: å¯¹æ¯ä¸ªæ—¶é—´å¸§çš„ç©ºé—´patchesåšæ³¨æ„åŠ›
        self.enc_spatial_transformer = Transformer(depth=spatial_depth, **transformer_kwargs)

        # 2. æ—¶é—´ç¼–ç å™¨: å¯¹åŒä¸€ç©ºé—´ä½ç½®çš„æ—¶é—´åºåˆ—åšæ³¨æ„åŠ›
        self.enc_temporal_transformer = Transformer(depth=temporal_depth, **transformer_kwargs)

        # ===== Vector Quantization =====
        # å°†è¿ç»­ç‰¹å¾é‡åŒ–ä¸ºç¦»æ•£çš„codebookç´¢å¼•
        self.vq = VectorQuantize(
            dim=dim,
            codebook_size=codebook_size,
            use_cosine_sim=True  # ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦è¿›è¡Œé‡åŒ–
        )

        # ===== è§£ç å™¨ (Decoder) =====
        # æ³¨æ„: åŸå§‹ä»£ç ç¼ºå°‘è§£ç å™¨å®šä¹‰ï¼Œè¿™é‡Œè¡¥å……å®Œæ•´
        # è§£ç å™¨ç»“æ„ä¸ç¼–ç å™¨å¯¹ç§°ï¼Œä½†é¡ºåºç›¸å: æ—¶é—´ -> ç©ºé—´

        # 1. æ—¶é—´è§£ç å™¨
        self.dec_temporal_transformer = Transformer(depth=temporal_depth, **transformer_kwargs)

        # 2. ç©ºé—´è§£ç å™¨
        self.dec_spatial_transformer = Transformer(depth=spatial_depth, **transformer_kwargs)

        # ===== åƒç´ é‡å»ºå±‚ =====
        # å°†patchesæ˜ å°„å›åƒç´ ç©ºé—´
        # Input:  (B, T', H', W', D)
        # Output: (B, C, T, H, W)
        self.to_pixels = nn.Sequential(
            # çº¿æ€§æŠ•å½±: D -> C*pt*pH*pW
            nn.Linear(dim, channels * patch_width * patch_height * temporal_patch_size),
            # Rearrange: é‡ç»„ä¸ºåŸå§‹å½¢çŠ¶
            # (B, T', H', W', C*pt*pH*pW) -> (B, C, T, H, W)
            Rearrange(
                'b t h w (c pt p1 p2) -> b c (t pt) (h p1) (w p2)',
                p1=patch_height, p2=patch_width, pt=temporal_patch_size
            ),
        )

    @property
    def patch_height_width(self):
        """è¿”å›patch gridçš„å°ºå¯¸ (H', W')"""
        return self.image_size[0] // self.patch_size[0], self.image_size[1] // self.patch_size[1]

    @property
    def image_num_tokens(self):
        """è¿”å›æ¯ä¸ªæ—¶é—´å¸§çš„tokenæ•°é‡"""
        return int(self.image_size[0] / self.patch_size[0]) * int(self.image_size[1] / self.patch_size[1])

    def encode(self, tokens: torch.Tensor) -> torch.Tensor:
        """
        ç¼–ç è¿‡ç¨‹: ç©ºé—´æ³¨æ„åŠ› -> æ—¶é—´æ³¨æ„åŠ›

        Args:
            tokens: (B, T', H', W', D) - Patch embeddings

        Returns:
            tokens: (B, T', H', W', D) - Encoded tokens
        """
        b = tokens.shape[0]
        h, w = self.patch_height_width

        video_shape = tuple(tokens.shape[:-1])  # (B, T', H', W')

        # ===== ç©ºé—´ç¼–ç  (Spatial Encoding) =====
        # å¯¹æ¯ä¸ªæ—¶é—´å¸§ç‹¬ç«‹åšç©ºé—´æ³¨æ„åŠ›
        # (B, T', H', W', D) -> (B*T', H'*W', D)
        tokens = rearrange(tokens, 'b t h w d -> (b t) (h w) d')

        # è®¡ç®—ç©ºé—´ä½ç½®åç½®
        attn_bias = self.spatial_rel_pos_bias(h, w, device=tokens.device)

        # ç©ºé—´Transformer
        tokens = self.enc_spatial_transformer(tokens, attn_bias=attn_bias, video_shape=video_shape)

        # Reshapeå›4D: (B*T', H'*W', D) -> (B, T', H', W', D)
        tokens = rearrange(tokens, '(b t) (h w) d -> b t h w d', b=b, h=h, w=w)

        # ===== æ—¶é—´ç¼–ç  (Temporal Encoding) =====
        # å¯¹åŒä¸€ç©ºé—´ä½ç½®çš„æ—¶é—´åºåˆ—åšæ³¨æ„åŠ›
        # (B, T', H', W', D) -> (B*H'*W', T', D)
        tokens = rearrange(tokens, 'b t h w d -> (b h w) t d')

        # æ—¶é—´Transformer
        tokens = self.enc_temporal_transformer(tokens, video_shape=video_shape)

        # Reshapeå›4D: (B*H'*W', T', D) -> (B, T', H', W', D)
        tokens = rearrange(tokens, '(b h w) t d -> b t h w d', b=b, h=h, w=w)

        return tokens

    def decode(self, tokens: torch.Tensor) -> torch.Tensor:
        """
        è§£ç è¿‡ç¨‹: æ—¶é—´æ³¨æ„åŠ› -> ç©ºé—´æ³¨æ„åŠ› -> åƒç´ é‡å»º

        æ³¨æ„: è§£ç é¡ºåºä¸ç¼–ç ç›¸å
            ç¼–ç : ç©ºé—´ -> æ—¶é—´
            è§£ç : æ—¶é—´ -> ç©ºé—´

        Args:
            tokens: (B, T', H', W', D) æˆ– (B, N, D) - Quantized tokens

        Returns:
            recon_video: (B, C, T, H, W) - é‡å»ºçš„video
        """
        b = tokens.shape[0]
        h, w = self.patch_height_width

        # å¦‚æœè¾“å…¥æ˜¯flattençš„ (B, N, D)ï¼Œå…ˆreshapeä¸º4D
        if tokens.ndim == 3:
            tokens = rearrange(tokens, 'b (t h w) d -> b t h w d', h=h, w=w)

        video_shape = tuple(tokens.shape[:-1])  # (B, T', H', W')

        # ===== æ—¶é—´è§£ç  (Temporal Decoding) =====
        # (B, T', H', W', D) -> (B*H'*W', T', D)
        tokens = rearrange(tokens, 'b t h w d -> (b h w) t d')

        # æ—¶é—´Transformer
        tokens = self.dec_temporal_transformer(tokens, video_shape=video_shape)

        # Reshape: (B*H'*W', T', D) -> (B, T', H', W', D)
        tokens = rearrange(tokens, '(b h w) t d -> b t h w d', b=b, h=h, w=w)

        # ===== ç©ºé—´è§£ç  (Spatial Decoding) =====
        # (B, T', H', W', D) -> (B*T', H'*W', D)
        tokens = rearrange(tokens, 'b t h w d -> (b t) (h w) d')

        # è®¡ç®—ç©ºé—´ä½ç½®åç½®
        attn_bias = self.spatial_rel_pos_bias(h, w, device=tokens.device)

        # ç©ºé—´Transformer
        tokens = self.dec_spatial_transformer(tokens, attn_bias=attn_bias, video_shape=video_shape)

        # Reshape: (B*T', H'*W', D) -> (B, T', H', W', D)
        tokens = rearrange(tokens, '(b t) (h w) d -> b t h w d', b=b, h=h, w=w)

        # ===== åƒç´ é‡å»º =====
        # (B, T', H', W', D) -> (B, C, T, H, W)
        recon_video = self.to_pixels(tokens)

        return recon_video

    def forward(
        self,
        video: torch.Tensor,
        mask: Optional[torch.Tensor] = None,
        return_recons_only: bool = False,
        return_only_codebook_ids: bool = False,
        return_encoded_tokens: bool = False
    ) -> Union[torch.Tensor, Tuple[torch.Tensor, ...]]:
        """
        å‰å‘ä¼ æ’­

        Args:
            video: (B, C, T, H, W) - è¾“å…¥CT volume
            mask: (B, T) - å¯é€‰çš„æ—¶é—´mask (True=ä¿ç•™, False=mask)
            return_recons_only: æ˜¯å¦åªè¿”å›é‡å»ºç»“æœ
            return_only_codebook_ids: æ˜¯å¦åªè¿”å›VQç´¢å¼•
            return_encoded_tokens: æ˜¯å¦åªè¿”å›ç¼–ç åçš„tokens

        Returns:
            é»˜è®¤: (recon_loss, commit_loss, recon_video)
            return_recons_only=True: recon_video
            return_only_codebook_ids=True: indices
            return_encoded_tokens=True: tokens

        Note:
            - è¾“å…¥å¿…é¡»æ˜¯5D tensor (videoæ ¼å¼)
            - åŸå§‹ä»£ç çš„is_imageåˆ†æ”¯å·²åˆ é™¤ï¼Œå› ä¸ºå®é™…ä½¿ç”¨ä¸­åªéœ€è¦videoè¾“å…¥
        """
        # ===== è¾“å…¥æ£€æŸ¥ =====
        assert video.ndim == 5, f"Input must be 5D (B, C, T, H, W), got shape {video.shape}"

        b, c, f, *image_dims, device = *video.shape, video.device

        # æ£€æŸ¥å›¾åƒå°ºå¯¸
        assert tuple(image_dims) == self.image_size, \
            f"Input image size {image_dims} doesn't match model image size {self.image_size}"

        # æ£€æŸ¥maskå°ºå¯¸
        assert not exists(mask) or mask.shape[-1] == f, \
            f"Mask temporal dimension {mask.shape[-1]} doesn't match video frames {f}"

        # ===== 1. Patch Embedding =====
        # (B, C, T, H, W) -> (B, T', H', W', D)
        tokens = self.to_patch_emb(video)

        # ä¿å­˜shapeä¿¡æ¯
        *_, h, w, _ = tokens.shape

        # ===== 2. ç¼–ç  (Spatial -> Temporal) =====
        tokens = self.encode(tokens)

        # ===== 3. Vector Quantization =====
        # Flatten tokens: (B, T', H', W', D) -> (B, T'*H'*W', D)
        tokens, packed_fhw_shape = pack([tokens], 'b * d')

        # è®¡ç®—VQ mask (å¦‚æœæä¾›äº†æ—¶é—´mask)
        vq_mask = None
        if exists(mask):
            vq_mask = self.calculate_video_token_mask(video, mask)

        # VQé‡åŒ–
        # tokens: é‡åŒ–åçš„è¿ç»­ç‰¹å¾
        # indices: codebookç´¢å¼•
        # commit_loss: VQæ‰¿è¯ºæŸå¤±
        tokens, indices, commit_loss = self.vq(tokens, mask=vq_mask)

        # å¦‚æœåªéœ€è¦è¿”å›codebookç´¢å¼•
        if return_only_codebook_ids:
            indices, = unpack(indices, packed_fhw_shape, 'b *')
            return indices

        # Reshapeå›4D: (B, T'*H'*W', D) -> (B, T', H', W', D)
        tokens = rearrange(tokens, 'b (t h w) d -> b t h w d', h=h, w=w)

        # å¦‚æœåªéœ€è¦è¿”å›ç¼–ç åçš„tokens
        if return_encoded_tokens:
            return tokens

        # ===== 4. è§£ç  (Temporal -> Spatial -> Pixels) =====
        recon_video = self.decode(tokens)

        # å¦‚æœåªéœ€è¦è¿”å›é‡å»ºç»“æœ
        if return_recons_only:
            return recon_video

        # ===== 5. è®¡ç®—æŸå¤± =====
        # é‡å»ºæŸå¤± (MSE)
        if exists(mask):
            # å¦‚æœæœ‰maskï¼Œåªè®¡ç®—émaskä½ç½®çš„æŸå¤±
            recon_loss = F.mse_loss(video, recon_video, reduction='none')
            # åº”ç”¨mask: (B, T) -> (B, C, T, 1, 1)
            mask_expanded = repeat(mask, 'b t -> b c t 1 1', c=c)
            recon_loss = recon_loss[mask_expanded]
            recon_loss = recon_loss.mean()
        else:
            # å…¨éƒ¨ä½ç½®éƒ½è®¡ç®—æŸå¤±
            recon_loss = F.mse_loss(video, recon_video)

        # ===== 6. è¿”å›ç»“æœ =====
        # è¿”å›: (é‡å»ºæŸå¤±, VQæ‰¿è¯ºæŸå¤±, é‡å»ºvideo)
        return recon_loss, commit_loss, recon_video

    def calculate_video_token_mask(self, videos: torch.Tensor, video_frame_mask: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—tokençº§åˆ«çš„mask (ç”¨äºVQ)

        å°†å¸§çº§åˆ«çš„maskè½¬æ¢ä¸ºtokençº§åˆ«çš„mask

        Args:
            videos: (B, C, T, H, W)
            video_frame_mask: (B, T) - å¸§çº§åˆ«mask

        Returns:
            token_mask: (B, N) - Tokençº§åˆ«mask, N = T' * H' * W'
        """
        *_, h, w = videos.shape
        ph, pw = self.patch_size

        # å°†å¸§maskæŒ‰temporal_patch_sizeåˆ†ç»„
        # å¦‚æœä¸€ç»„å†…æœ‰ä»»ä½•å¸§ä¸ºTrueï¼Œåˆ™è¯¥patchä¸ºTrue
        rest_vq_mask = rearrange(video_frame_mask, 'b (f p) -> b f p', p=self.temporal_patch_size)
        video_mask = rest_vq_mask.any(dim=-1)  # (B, T')

        # æ‰©å±•åˆ°æ‰€æœ‰ç©ºé—´ä½ç½®
        # (B, T') -> (B, T' * H' * W')
        return repeat(video_mask, 'b f -> b (f hw)', hw=(h // ph) * (w // pw))

    def copy_for_eval(self):
        """
        åˆ›å»ºæ¨¡å‹çš„è¯„ä¼°å‰¯æœ¬

        ç”¨äºä¿å­˜/éƒ¨ç½²æ—¶å»é™¤è®­ç»ƒç›¸å…³ç»„ä»¶

        Returns:
            vae_copy: è¯„ä¼°æ¨¡å¼çš„æ¨¡å‹å‰¯æœ¬
        """
        device = next(self.parameters()).device
        vae_copy = copy.deepcopy(self.cpu())
        vae_copy.eval()
        return vae_copy.to(device)

    def load(self, path: Union[str, Path]):
        """
        ä»checkpointåŠ è½½æ¨¡å‹æƒé‡

        Args:
            path: checkpointæ–‡ä»¶è·¯å¾„
        """
        path = Path(path)
        assert path.exists(), f"Checkpoint not found: {path}"
        pt = torch.load(str(path))
        self.load_state_dict(pt)

    def decode_from_codebook_indices(self, indices: torch.Tensor) -> torch.Tensor:
        """
        ä»codebookç´¢å¼•ç›´æ¥è§£ç 

        ç”¨äºä»ç¦»æ•£ç´¢å¼•é‡å»ºvideo

        Args:
            indices: (B, N) - Codebookç´¢å¼•

        Returns:
            recon_video: (B, C, T, H, W) - é‡å»ºçš„video
        """
        # ä»codebookè·å–å¯¹åº”çš„ç‰¹å¾å‘é‡
        codes = self.vq.codebook[indices]
        # è§£ç 
        return self.decode(codes)

    def num_tokens_per_frames(self, num_frames: int, include_first_frame: bool = True) -> int:
        """
        è®¡ç®—ç»™å®šå¸§æ•°å¯¹åº”çš„tokenæ•°é‡

        Args:
            num_frames: å¸§æ•°
            include_first_frame: æ˜¯å¦åŒ…å«ç¬¬ä¸€å¸§ (å…¼å®¹æ—§ä»£ç ï¼Œå®é™…ä¸Šå·²ä¸åŒºåˆ†)

        Returns:
            total_tokens: Tokenæ€»æ•°
        """
        image_num_tokens = self.image_num_tokens

        # æ£€æŸ¥å¸§æ•°èƒ½å¦è¢«temporal_patch_sizeæ•´é™¤
        assert (num_frames % self.temporal_patch_size) == 0, \
            f"num_frames {num_frames} must be divisible by temporal_patch_size {self.temporal_patch_size}"

        # è®¡ç®—: (T / temporal_patch_size) * (H' * W')
        return int(num_frames / self.temporal_patch_size) * image_num_tokens
