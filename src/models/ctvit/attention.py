"""
CTViT æ³¨æ„åŠ›æ¨¡å— (Attention Modules)

åŒ…å«ï¼š
- Attention (å¤šå¤´è‡ªæ³¨æ„åŠ›)
- AlibiPositionalBias (ALiBiä½ç½®åç½®)
- ContinuousPositionBias (è¿ç»­ä½ç½®åç½®)
- Transformer (å®Œæ•´Transformerå—)
"""

import math
import torch
import torch.nn.functional as F
from torch import nn, einsum
from einops import rearrange, repeat
from beartype import beartype
from typing import Tuple, Optional

from .layers import (
    exists, default, leaky_relu, l2norm,
    LayerNorm, RMSNorm, GEGLU, SwiGLU, FeedForward, PEG
)
from flash_attn.flash_attn_interface import (
    flash_attn_varlen_qkvpacked_func
)

from flash_attn.flash_attn_interface import flash_attn_qkvpacked_func

class FlashAttentionQKV(nn.Module):
    """
    Clean FlashAttention v2 module for CT-ViT.
    """

    def __init__(
        self,
        dim,
        dim_context=None,
        dim_head=64,
        heads=8,
        causal=False,
        num_null_kv=0,
        dropout=0.
    ):
        super().__init__()
        self.heads = heads
        self.dim_head = dim_head
        self.inner_dim = dim_head * heads
        self.causal = causal
        self.num_null_kv = num_null_kv
        self.dropout = dropout

        dim_context = dim if dim_context is None else dim_context

        self.norm = nn.LayerNorm(dim)
        self.context_norm = nn.LayerNorm(dim_context)

        self.to_q = nn.Linear(dim, self.inner_dim, bias=False)
        self.to_kv = nn.Linear(dim_context, self.inner_dim * 2, bias=False)

        # null kv
        self.null_kv = nn.Parameter(torch.randn(heads, num_null_kv, 2, dim_head))

        self.to_out = nn.Linear(self.inner_dim, dim, bias=False)

    def forward(self, x, mask=None, context=None):
        b, n, device = x.shape[0], x.shape[1], x.device

        if context is not None:
            context = self.context_norm(context)

        kv_input = context if context is not None else x
        x = self.norm(x)

        # project
        q = self.to_q(x)
        k, v = self.to_kv(kv_input).chunk(2, dim=-1)

        # reshape: (b, n, h*d) -> (b, n, h, d)
        q = q.view(b, -1, self.heads, self.dim_head)
        k = k.view(b, -1, self.heads, self.dim_head)
        v = v.view(b, -1, self.heads, self.dim_head)

        # add null kv if exists
        if self.num_null_kv > 0:
            nk = self.null_kv[:, :, 0, :]  # (H, N, D)
            nv = self.null_kv[:, :, 1, :]

            # Reshape to (B, num_null, H, D) for concatenation
            nk = nk.permute(1, 0, 2).unsqueeze(0).expand(b, -1, -1, -1)  # (B, N, H, D)
            nv = nv.permute(1, 0, 2).unsqueeze(0).expand(b, -1, -1, -1)

            k = torch.cat((nk, k), dim=1)  # concat on sequence dim
            v = torch.cat((nv, v), dim=1)

        # For flash_attn_qkvpacked_func, q/k/v must have same sequence length
        # When using null_kv, we need to use the unpacked version instead
        if self.num_null_kv > 0:
            # Use unpacked version for different q/kv lengths
            from flash_attn import flash_attn_func
            out = flash_attn_func(
                q.half(),
                k.half(),
                v.half(),
                dropout_p=self.dropout if self.training else 0.0,
                causal=self.causal
            )
        else:
            # pack to qkv for flash-attn: (b, seqlen, 3, h, d)
            qkv = torch.stack([q, k, v], dim=2)

            out = flash_attn_qkvpacked_func(
                qkv.half(),
                dropout_p=self.dropout if self.training else 0.0,
                causal=self.causal
            )

        # reshape back
        out = out.view(b, -1, self.inner_dim)
        return self.to_out(out.to(x.dtype))


# ============================================================================
# Attention (å¤šå¤´è‡ªæ³¨æ„åŠ›)
# ============================================================================

class Attention(nn.Module):
    """
    Multi-Head Self-Attention (å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶)

    ç‰¹ç‚¹ï¼š
    1. QK Normalization: Qå’ŒKå‘é‡è¿›è¡ŒL2å½’ä¸€åŒ–ï¼Œæå‡è®­ç»ƒç¨³å®šæ€§
    2. Learnable Scale: ä¸ºQå’ŒKæ·»åŠ å¯å­¦ä¹ çš„ç¼©æ”¾å‚æ•°
    3. Null Key-Value: é¢å¤–çš„å¯å­¦ä¹ KVå¯¹ï¼Œå¢å¼ºè¡¨è¾¾èƒ½åŠ›
    4. æ”¯æŒCross-Attention: å¯æ¥å—å¤–éƒ¨context
    5. æ”¯æŒCausal Attention: ç”¨äºè‡ªå›å½’ç”Ÿæˆ

    è®¡ç®—æµç¨‹:
        1. RMSNorm(x) -> Q, K, V
        2. L2 Normalize Q, K
        3. Attention = softmax(Q @ K^T * scale) @ V
        4. Linear projection

    Args:
        dim: è¾“å…¥ç‰¹å¾ç»´åº¦
        dim_context: Contextç»´åº¦ (ç”¨äºcross-attention)
        dim_head: æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„ç»´åº¦ (é»˜è®¤64)
        heads: æ³¨æ„åŠ›å¤´æ•° (é»˜è®¤8)
        causal: æ˜¯å¦ä½¿ç”¨å› æœæ³¨æ„åŠ› (é»˜è®¤False)
        num_null_kv: Null key-valueå¯¹çš„æ•°é‡ (é»˜è®¤0)
        norm_context: æ˜¯å¦å¯¹contextè¿›è¡Œå½’ä¸€åŒ– (é»˜è®¤True)
        dropout: Dropoutæ¯”ç‡ (é»˜è®¤0)
        scale: æ³¨æ„åŠ›ç¼©æ”¾å› å­ (é»˜è®¤8)

    ğŸ”§ [ç°ä»£åŒ–æ”¹é€ ç‚¹] å¯ä»¥å‡çº§ä¸ºï¼š
    1. Flash Attention 2.0:
       - ä½¿ç”¨èåˆCUDA kernelï¼Œå¤§å¹…å‡å°‘å†…å­˜è®¿é—®
       - åŠ é€Ÿ2-4å€ï¼Œæ”¯æŒæ›´é•¿åºåˆ—
       - å®ç°: æ›¿æ¢ einsum + softmax ä¸º flash_attn_func()

    2. Grouped-Query Attention (GQA):
       - å¤šä¸ªQuery headå…±äº«ä¸€ç»„KV head
       - å‡å°‘KV cacheï¼ŒåŠ é€Ÿæ¨ç†
       - ä¾‹å¦‚: 8ä¸ªQ head, 2ä¸ªKV head (4:1æ¯”ä¾‹)

    3. Multi-Query Attention (MQA):
       - æ‰€æœ‰Query headå…±äº«1ç»„KV
       - æœ€å¤§åŒ–æ¨ç†é€Ÿåº¦

    4. Sliding Window Attention:
       - åªå…³æ³¨å±€éƒ¨çª—å£ï¼Œå‡å°‘è®¡ç®—å¤æ‚åº¦
       - é€‚åˆè¶…é•¿åºåˆ—
    """

    def __init__(
        self,
        dim,
        dim_context=None,
        dim_head=64,
        heads=8,
        causal=False,
        num_null_kv=0,
        norm_context=True,
        dropout=0.,
        scale=8,
        use_rms_norm=False
    ):
        super().__init__()
        self.heads = heads
        self.causal = causal
        self.scale = scale
        inner_dim = dim_head * heads
        dim_context = default(dim_context, dim)

        # å¦‚æœæ˜¯å› æœæ³¨æ„åŠ›ï¼Œä½¿ç”¨ALiBiä½ç½®åç½®
        if causal:
            self.rel_pos_bias = AlibiPositionalBias(heads=heads)

        self.attn_dropout = nn.Dropout(dropout)

        # Normalization layers (configurable: LayerNorm for baseline, RMSNorm for optimized)
        norm_class = RMSNorm if use_rms_norm else LayerNorm
        self.norm = norm_class(dim)
        self.context_norm = norm_class(dim_context) if norm_context else nn.Identity()

        # Null Key-Value pairs (é¢å¤–çš„å¯å­¦ä¹ KVï¼Œå¢å¼ºè¡¨è¾¾èƒ½åŠ›)
        self.num_null_kv = num_null_kv
        self.null_kv = nn.Parameter(torch.randn(heads, 2 * num_null_kv, dim_head))

        # Q, K, V projection
        self.to_q = nn.Linear(dim, inner_dim, bias=False)
        self.to_kv = nn.Linear(dim_context, inner_dim * 2, bias=False)

        # QK Normalizationçš„å¯å­¦ä¹ ç¼©æ”¾å‚æ•°
        # æå‡è®­ç»ƒç¨³å®šæ€§ï¼Œé˜²æ­¢softmaxé¥±å’Œ
        self.q_scale = nn.Parameter(torch.ones(dim_head))
        self.k_scale = nn.Parameter(torch.ones(dim_head))

        # Output projection
        self.to_out = nn.Linear(inner_dim, dim, bias=False)

    def forward(
        self,
        x,
        mask=None,
        context=None,
        attn_bias=None
    ):
        """
        Args:
            x: è¾“å…¥ç‰¹å¾ (B, N, D)
            mask: æ³¨æ„åŠ›mask (B, N) - Trueè¡¨ç¤ºä¿ç•™ï¼ŒFalseè¡¨ç¤ºmaskæ‰
            context: å¤–éƒ¨contextç”¨äºcross-attention (B, M, D_ctx)
            attn_bias: é¢å¤–çš„æ³¨æ„åŠ›åç½® (H, N, N) å¦‚ä½ç½®ç¼–ç 

        Returns:
            è¾“å‡ºç‰¹å¾ (B, N, D)
        """
        batch, device, dtype = x.shape[0], x.device, x.dtype

        # Normalize context (å¦‚æœæœ‰)
        if exists(context):
            context = self.context_norm(context)

        # é€‰æ‹©KVæ¥æº: context (cross-attn) æˆ– x (self-attn)
        kv_input = default(context, x)

        # Normalize input
        x = self.norm(x)

        # è®¡ç®— Q, K, V
        q, k, v = self.to_q(x), *self.to_kv(kv_input).chunk(2, dim=-1)

        # Reshapeä¸ºå¤šå¤´: (B, N, H*D) -> (B, H, N, D)
        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.heads), (q, k, v))

        # æ·»åŠ Null Key-Value pairs

        if self.num_null_kv > 0:
            null = self.null_kv
            null = null.view(self.heads, self.num_null_kv, 2, self.dim_head)
            null = null.unsqueeze(0).expand(batch, -1, -1, -1, -1)

            nk = null[..., 0, :]
            nv = null[..., 1, :]

            k = torch.cat((nk, k), dim=-2)
            v = torch.cat((nv, v), dim=-2)

        # QK Normalization (æå‡è®­ç»ƒç¨³å®šæ€§)
        q, k = map(l2norm, (q, k))
        q = q * self.q_scale  # å¯å­¦ä¹ ç¼©æ”¾
        k = k * self.k_scale

        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°: Q @ K^T
        sim = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale

        i, j = sim.shape[-2:]

        # æ·»åŠ ä½ç½®ç¼–ç åç½® (å¦‚æœæœ‰)
        # if exists(attn_bias):
        #     # ä¸ºnull_kvéƒ¨åˆ†padding 0
        #     attn_bias = F.pad(attn_bias, (self.num_null_kv, 0), value=0.)
        #     sim = sim + attn_bias

        # åº”ç”¨attention mask (å¦‚æœæœ‰)
        if exists(mask):
            # ä¸ºnull_kvéƒ¨åˆ†padding True (ä¸mask)
            mask = F.pad(mask, (self.num_null_kv, 0), value=True)
            mask = rearrange(mask, 'b j -> b 1 1 j')
            # maskæ‰çš„ä½ç½®å¡«å……ä¸º-infï¼Œsoftmaxåå˜æˆ0
            sim = sim.masked_fill(~mask, -torch.finfo(sim.dtype).max)

        # å› æœæ³¨æ„åŠ›mask (å¦‚æœéœ€è¦)
        if self.causal:
            # æ·»åŠ ALiBiä½ç½®åç½®
            sim = sim + self.rel_pos_bias(sim)
            # åˆ›å»ºä¸Šä¸‰è§’mask (åªèƒ½çœ‹åˆ°è¿‡å»å’Œå½“å‰)
            causal_mask = torch.ones((i, j), device=device, dtype=torch.bool).triu(j - i + 1)
            sim = sim.masked_fill(causal_mask, -torch.finfo(sim.dtype).max)

        # Softmaxè®¡ç®—æ³¨æ„åŠ›æƒé‡
        attn = sim.softmax(dim=-1)
        attn = self.attn_dropout(attn)

        # åº”ç”¨æ³¨æ„åŠ›æƒé‡åˆ°V: Attention @ V
        out = einsum('b h i j, b h j d -> b h i d', attn, v)

        # åˆå¹¶å¤šå¤´: (B, H, N, D) -> (B, N, H*D)
        out = rearrange(out, 'b h n d -> b n (h d)')

        # è¾“å‡ºæŠ•å½±
        return self.to_out(out)


# ============================================================================
# ALiBi Positional Bias (ALiBiä½ç½®åç½®)
# ============================================================================

class AlibiPositionalBias(nn.Module):
    """
    ALiBi (Attention with Linear Biases) ä½ç½®åç½®

    è®ºæ–‡: Train Short, Test Long: Attention with Linear Biases Enables
          Input Length Extrapolation

    åŸç†:
        - ä¸ä½¿ç”¨ä½ç½®ç¼–ç ï¼Œè€Œæ˜¯åœ¨attention scoreä¸Šæ·»åŠ çº¿æ€§åç½®
        - åç½®éšè·ç¦»çº¿æ€§å¢é•¿ï¼Œè·ç¦»è¶Šè¿œæƒ©ç½šè¶Šå¤§
        - æ¯ä¸ªæ³¨æ„åŠ›å¤´ä½¿ç”¨ä¸åŒçš„æ–œç‡ (slope)

    ä¼˜ç‚¹:
        1. å¤–æ¨èƒ½åŠ›å¼ºï¼šè®­ç»ƒçŸ­åºåˆ—ï¼Œæ¨ç†æ—¶å¯ä»¥å¤„ç†æ›´é•¿åºåˆ—
        2. ç®€å•é«˜æ•ˆï¼šä¸éœ€è¦å¤æ‚çš„ä½ç½®ç¼–ç 
        3. æ— éœ€é¢å¤–å‚æ•°

    å…¬å¼:
        bias[i, j] = -slope * |i - j|
        å…¶ä¸­slopeå¯¹æ¯ä¸ªå¤´ä¸åŒï¼ŒæŒ‰2çš„å¹‚æ¬¡é€’å‡

    Args:
        heads: æ³¨æ„åŠ›å¤´æ•°

    ğŸ”§ [ç°ä»£åŒ–æ”¹é€ ç‚¹] ç›¸å…³æ›¿ä»£æ–¹æ¡ˆï¼š
    1. RoPE (Rotary Position Embedding):
       - é€šè¿‡æ—‹è½¬å˜æ¢ç¼–ç ä½ç½®ä¿¡æ¯
       - å¤–æ¨èƒ½åŠ›ä¹Ÿå¾ˆå¥½
       - è¢«LLaMAç­‰æ¨¡å‹é‡‡ç”¨

    2. xPos (Extrapolatable Position Embedding):
       - ALiBiçš„æ”¹è¿›ç‰ˆ
       - æ›´å¥½çš„å¤–æ¨æ€§èƒ½
    """

    def __init__(self, heads):
        super().__init__()
        self.heads = heads
        # è®¡ç®—æ¯ä¸ªå¤´çš„slope
        slopes = torch.Tensor(self._get_slopes(heads))
        slopes = rearrange(slopes, 'h -> h 1 1')
        # æ³¨å†Œä¸ºbuffer (ä¸å‚ä¸è®­ç»ƒï¼Œä½†ä¼šéšæ¨¡å‹ä¿å­˜/åŠ è½½)
        self.register_buffer('slopes', slopes, persistent=False)
        self.register_buffer('bias', None, persistent=False)

    def get_bias(self, i, j, device):
        """
        ç”Ÿæˆä½ç½®åç½®çŸ©é˜µ

        Args:
            i: queryåºåˆ—é•¿åº¦
            j: keyåºåˆ—é•¿åº¦
            device: è®¾å¤‡

        Returns:
            bias: (1, i, j) - ä½ç½®åç½®çŸ©é˜µ
        """
        # ç”Ÿæˆposition indices
        i_arange = torch.arange(j - i, j, device=device)  # query positions
        j_arange = torch.arange(j, device=device)          # key positions

        # è®¡ç®—è·ç¦»çŸ©é˜µ: |i - j|
        bias = -torch.abs(
            rearrange(j_arange, 'j -> 1 1 j') -
            rearrange(i_arange, 'i -> 1 i 1')
        )
        return bias

    @staticmethod
    def _get_slopes(heads):
        """
        è®¡ç®—æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„slope

        ç­–ç•¥: æŒ‰2çš„å¹‚æ¬¡é€’å‡
            - å¦‚æœheads=8: slopes = [2^-1, 2^-2, ..., 2^-8]
        """
        def get_slopes_power_of_2(n):
            start = (2**(-2**-(math.log2(n)-3)))
            ratio = start
            return [start*ratio**i for i in range(n)]

        # å¦‚æœheadsæ˜¯2çš„å¹‚
        if math.log2(heads).is_integer():
            return get_slopes_power_of_2(heads)

        # å¦‚æœä¸æ˜¯ï¼Œå–æœ€æ¥è¿‘çš„2çš„å¹‚ï¼Œç„¶åæ’å€¼
        closest_power_of_2 = 2 ** math.floor(math.log2(heads))
        return (get_slopes_power_of_2(closest_power_of_2) +
                get_slopes_power_of_2(2 * closest_power_of_2)[0::2][:heads-closest_power_of_2])

    def forward(self, sim):
        """
        Args:
            sim: æ³¨æ„åŠ›åˆ†æ•° (B, H, i, j)

        Returns:
            ALiBiåç½® (H, i, j)
        """
        h, i, j, device = *sim.shape[-3:], sim.device

        # å¦‚æœå·²ç¼“å­˜ä¸”å°ºå¯¸è¶³å¤Ÿå¤§ï¼Œç›´æ¥ä½¿ç”¨
        if exists(self.bias) and self.bias.shape[-1] >= j:
            return self.bias[..., :i, :j]

        # ç”Ÿæˆbias
        bias = self.get_bias(i, j, device)
        # ä¹˜ä»¥æ¯ä¸ªå¤´çš„slope
        bias = bias * self.slopes

        # å¦‚æœheadsæ•°é‡å¤§äºå·²è®¡ç®—çš„biaså¤´æ•°ï¼Œpadding 0
        num_heads_unalibied = h - bias.shape[0]
        bias = F.pad(bias, (0, 0, 0, 0, 0, num_heads_unalibied))

        # ç¼“å­˜èµ·æ¥
        self.register_buffer('bias', bias, persistent=False)

        return self.bias


# ============================================================================
# Continuous Position Bias (è¿ç»­ä½ç½®åç½®)
# ============================================================================

class ContinuousPositionBias(nn.Module):
    """
    Continuous Position Bias (è¿ç»­ä½ç½®åç½®)

    è®ºæ–‡: "Conditional Positional Encodings for Vision Transformers"

    åŸç†:
        - ä½¿ç”¨å°å‹MLPå°†ç›¸å¯¹ä½ç½®åæ ‡æ˜ å°„ä¸ºæ³¨æ„åŠ›åç½®
        - æ”¯æŒ2D (å›¾åƒ) å’Œ 3D (è§†é¢‘) ä½ç½®ç¼–ç 
        - ä½¿ç”¨å¯¹æ•°è·ç¦»ç¼–ç ï¼Œå¢å¼ºè¿œè·ç¦»å»ºæ¨¡

    ç»“æ„:
        Relative Position Coords
        â†’ MLP (Linear + LeakyReLU) Ã— layers
        â†’ Linear(heads)
        â†’ Attention Bias

    Args:
        dim: MLPéšè—ç»´åº¦
        heads: æ³¨æ„åŠ›å¤´æ•°
        num_dims: ä½ç½®ç»´åº¦æ•° (2=å›¾åƒ, 3=è§†é¢‘)
        layers: MLPå±‚æ•°
        log_dist: æ˜¯å¦ä½¿ç”¨å¯¹æ•°è·ç¦» (é»˜è®¤True)
        cache_rel_pos: æ˜¯å¦ç¼“å­˜ç›¸å¯¹ä½ç½® (é»˜è®¤False)

    ğŸ”§ [ç°ä»£åŒ–æ”¹é€ ç‚¹] ç›¸å…³æ›¿ä»£æ–¹æ¡ˆï¼š
    1. 2D RoPE: å°†RoPEæ‰©å±•åˆ°2Dï¼Œä¸ºHå’ŒWç»´åº¦åˆ†åˆ«åº”ç”¨æ—‹è½¬
    2. å¯å­¦ä¹ çš„2D Sinusoidal: å°†å›ºå®šsin/cosä½ç½®ç¼–ç æ”¹ä¸ºå¯å­¦ä¹ 
    3. ç®€åŒ–MLP: å‡å°‘å±‚æ•°æˆ–ä½¿ç”¨æ›´è½»é‡çš„ç½‘ç»œ
    """

    def __init__(
        self,
        *,
        dim,
        heads,
        num_dims=2,  # 2 for images, 3 for video
        layers=2,
        log_dist=True,
        cache_rel_pos=False
    ):
        super().__init__()
        self.num_dims = num_dims
        self.log_dist = log_dist

        # æ„å»ºMLP
        self.net = nn.ModuleList([])
        # è¾“å…¥å±‚: ä»ä½ç½®åæ ‡(num_dims)æ˜ å°„åˆ°éšè—ç»´åº¦
        self.net.append(nn.Sequential(nn.Linear(self.num_dims, dim), leaky_relu()))

        # ä¸­é—´å±‚
        for _ in range(layers - 1):
            self.net.append(nn.Sequential(nn.Linear(dim, dim), leaky_relu()))

        # è¾“å‡ºå±‚: æ˜ å°„åˆ°æ¯ä¸ªæ³¨æ„åŠ›å¤´
        self.net.append(nn.Linear(dim, heads))

        self.cache_rel_pos = cache_rel_pos
        self.register_buffer('rel_pos', None, persistent=False)

    def forward(self, *dimensions, device=torch.device('cpu')):
        """
        Args:
            *dimensions: å„ç»´åº¦å¤§å°ï¼Œä¾‹å¦‚ (H, W) æˆ– (T, H, W)
            device: è®¾å¤‡

        Returns:
            ä½ç½®åç½® (H, i, j) å…¶ä¸­ i=j=H*W æˆ– T*H*W
        """
        # å¦‚æœæœªç¼“å­˜æˆ–ä¸ä½¿ç”¨ç¼“å­˜ï¼Œé‡æ–°è®¡ç®—
        if not exists(self.rel_pos) or not self.cache_rel_pos:
            # ç”Ÿæˆå„ç»´åº¦çš„position indices
            # ä¾‹å¦‚: H=3, W=4 -> positions = [range(3), range(4)]
            positions = [torch.arange(d, device=device) for d in dimensions]

            # ç”Ÿæˆç½‘æ ¼åæ ‡
            # grid.shape = (num_dims, *dimensions)
            # ä¾‹å¦‚: (2, 3, 4) -> [[0,0,0,0,1,1,1,1,2,2,2,2], [0,1,2,3,0,1,2,3,0,1,2,3]]
            grid = torch.stack(torch.meshgrid(*positions, indexing='ij'))
            grid = rearrange(grid, 'c ... -> (...) c')  # (HW, num_dims)

            # è®¡ç®—ç›¸å¯¹ä½ç½®: pos[i] - pos[j]
            # rel_pos.shape = (i, j, num_dims)
            rel_pos = rearrange(grid, 'i c -> i 1 c') - rearrange(grid, 'j c -> 1 j c')

            # å¯¹æ•°è·ç¦»ç¼–ç : sign(x) * log(|x| + 1)
            # è®©è¿œè·ç¦»çš„åŒºåˆ†åº¦é™ä½ï¼Œæ›´å…³æ³¨è¿‘è·ç¦»
            if self.log_dist:
                rel_pos = torch.sign(rel_pos) * torch.log(rel_pos.abs() + 1)

            # ç¼“å­˜
            self.register_buffer('rel_pos', rel_pos, persistent=False)

        # è½¬ä¸ºfloat32 (MLPè®¡ç®—)
        rel_pos = self.rel_pos.to(torch.float32)

        # é€šè¿‡MLP: (i, j, num_dims) -> (i, j, heads)
        for layer in self.net:
            rel_pos = layer(rel_pos.float())

        # è½¬æ¢ç»´åº¦é¡ºåº: (i, j, heads) -> (heads, i, j)
        return rearrange(rel_pos, 'i j h -> h i j')


# ============================================================================
# Transformer (å®Œæ•´Transformerå—)
# ============================================================================

class Transformer(nn.Module):
    """
    Transformeræ¨¡å— (å¤šå±‚å †å )

    ç»“æ„ (æ¯å±‚):
        Input
        â†’ PEG (ä½ç½®ç¼–ç , å¯é€‰)
        â†’ Self-Attention + Residual
        â†’ Cross-Attention + Residual (å¯é€‰)
        â†’ FeedForward + Residual
        â†’ Output

    Args:
        dim: ç‰¹å¾ç»´åº¦
        depth: Transformerå±‚æ•°
        dim_context: Contextç»´åº¦ (ç”¨äºcross-attention)
        causal: æ˜¯å¦ä½¿ç”¨å› æœæ³¨æ„åŠ›
        dim_head: æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„ç»´åº¦
        heads: æ³¨æ„åŠ›å¤´æ•°
        ff_mult: FeedForwardæ‰©å±•å€æ•°
        peg: æ˜¯å¦ä½¿ç”¨PEGä½ç½®ç¼–ç 
        peg_causal: PEGæ˜¯å¦ä½¿ç”¨å› æœpadding
        attn_num_null_kv: Null key-valueå¯¹æ•°é‡
        has_cross_attn: æ˜¯å¦åŒ…å«cross-attention
        attn_dropout: æ³¨æ„åŠ›dropout
        ff_dropout: FeedForward dropout

    ğŸ”§ [ç°ä»£åŒ–æ”¹é€ ç‚¹] æ•´ä½“æ¶æ„ä¼˜åŒ–ï¼š
    1. Pre-LN vs Post-LN:
       - å½“å‰: Post-LN (LNåœ¨Attentionå†…éƒ¨)
       - æ”¹ä¸ºPre-LN: LN(x) + Attn(...) æ›´ç¨³å®š
       - å‚è€ƒ: GPT-3, LLaMA

    2. Parallel Attention + FFN:
       - å°†Attentionå’ŒFFNå¹¶è¡Œè®¡ç®—åç›¸åŠ 
       - åŠ é€Ÿ10-15%ï¼Œæ€§èƒ½ç›¸å½“
       - å‚è€ƒ: PaLM

    3. MOE (Mixture of Experts):
       - å°†FFNæ”¹ä¸ºå¤šä¸ªä¸“å®¶çš„æ··åˆ
       - å¢åŠ å‚æ•°é‡ä½†ä¿æŒè®¡ç®—é‡
       - å‚è€ƒ: Switch Transformer
    """

    def __init__(
        self,
        dim,
        *,
        depth,
        dim_context=None,
        causal=False,
        dim_head=64,
        heads=8,
        ff_mult=4,
        peg=False,
        peg_causal=False,
        attn_num_null_kv=2,
        has_cross_attn=False,
        attn_dropout=0.,
        ff_dropout=0.,
        # NEW: Optimization flags
        use_flash_attention=False,
        use_rms_norm=False,
        use_swiglu=False
    ):
        super().__init__()
        self.layers = nn.ModuleList([])

        # Choose attention class based on optimization flag
        attn_class = FlashAttentionQKV if use_flash_attention else Attention

        # Choose normalization class based on optimization flag
        norm_class = RMSNorm if use_rms_norm else LayerNorm

        # å †å depthå±‚
        for _ in range(depth):
            # Build attention layers with appropriate parameters
            if use_flash_attention:
                # FlashAttentionQKV doesn't need use_rms_norm (it has hardcoded LayerNorm)
                self_attn = FlashAttentionQKV(
                    dim=dim, dim_head=dim_head, heads=heads,
                    causal=causal, dropout=attn_dropout
                )
                cross_attn = FlashAttentionQKV(
                    dim=dim, dim_head=dim_head, dim_context=dim_context,
                    heads=heads, causal=False, num_null_kv=attn_num_null_kv,
                    dropout=attn_dropout
                ) if has_cross_attn else None
            else:
                # Attention class needs use_rms_norm parameter
                self_attn = Attention(
                    dim=dim, dim_head=dim_head, heads=heads,
                    causal=causal, dropout=attn_dropout,
                    use_rms_norm=use_rms_norm
                )
                cross_attn = Attention(
                    dim=dim, dim_head=dim_head, dim_context=dim_context,
                    heads=heads, causal=False, num_null_kv=attn_num_null_kv,
                    dropout=attn_dropout,
                    use_rms_norm=use_rms_norm
                ) if has_cross_attn else None

            self.layers.append(nn.ModuleList([
                # 1. PEG (ä½ç½®ç¼–ç ç”Ÿæˆå™¨, å¯é€‰)
                PEG(dim=dim, causal=peg_causal) if peg else None,

                # 2. Self-Attention
                self_attn,

                # 3. Cross-Attention (å¯é€‰)
                cross_attn,

                # 4. FeedForward (with configurable activation)
                FeedForward(dim=dim, mult=ff_mult, dropout=ff_dropout, use_swiglu=use_swiglu)
            ]))

        # Output normalization (configurable)
        self.norm_out = norm_class(dim)


    @beartype
    def forward(
        self,
        x,
        video_shape: Tuple[int, int, int, int] = None,
        context=None,
        self_attn_mask=None,
        cross_attn_context_mask=None,
        attn_bias=None
    ):


        """
        Args:
            x: è¾“å…¥ç‰¹å¾ (B, N, D)
            video_shape: ç”¨äºPEGçš„å½¢çŠ¶ (B, T, H, W)
            attn_bias: æ³¨æ„åŠ›åç½®
            context: Cross-attentionçš„context
            self_attn_mask: Self-attentionçš„mask
            cross_attn_context_mask: Cross-attentionçš„mask

        Returns:
            è¾“å‡ºç‰¹å¾ (B, N, D)
        """
        # éå†æ¯ä¸€å±‚
        for peg, self_attn, cross_attn, ff in self.layers:
            # 1. ä½ç½®ç¼–ç  (å¦‚æœæœ‰)
            if exists(peg):
                x = peg(x, shape=video_shape) + x

            # 2. Self-Attention + Residual
            x = self_attn(x, mask=self_attn_mask) + x


            # 3. Cross-Attention + Residual (å¦‚æœæœ‰)
            if exists(cross_attn) and exists(context):
                x = cross_attn(x, context=context, mask=cross_attn_context_mask) + x


            # 4. FeedForward + Residual
            x = ff(x) + x

        # è¾“å‡ºå½’ä¸€åŒ–
        return self.norm_out(x)
