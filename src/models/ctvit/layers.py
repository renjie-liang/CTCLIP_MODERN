"""
CTViT åŸºç¡€å±‚ç»„ä»¶ (Basic Layer Components)

åŒ…å«ï¼š
- Helper functions (è¾…åŠ©å‡½æ•°)
- LayerNorm (å±‚å½’ä¸€åŒ– - baseline)
- RMSNorm (å±‚å½’ä¸€åŒ– - optimized)
- GEGLU activation (é—¨æ§æ¿€æ´»å‡½æ•° - baseline)
- SwiGLU activation (é—¨æ§æ¿€æ´»å‡½æ•° - optimized)
- FeedForward (å‰é¦ˆç½‘ç»œ - configurable)
- PEG (ä½ç½®ç¼–ç ç”Ÿæˆå™¨)
"""

import math
import torch
import torch.nn.functional as F
from torch import nn
from einops import rearrange
from beartype import beartype
from typing import Tuple, Optional



# ============================================================================
# Helper Functions (è¾…åŠ©å‡½æ•°)
# ============================================================================

def exists(val):
    """æ£€æŸ¥å€¼æ˜¯å¦å­˜åœ¨ (ä¸ä¸ºNone)"""
    return val is not None


def default(val, d):
    """å¦‚æœå€¼ä¸å­˜åœ¨ï¼Œè¿”å›é»˜è®¤å€¼"""
    return val if exists(val) else d


def pair(val):
    """
    å°†å•ä¸ªå€¼è½¬æ¢ä¸ºpair
    ä¾‹å¦‚: 480 -> (480, 480)
    """
    ret = (val, val) if not isinstance(val, tuple) else val
    assert len(ret) == 2
    return ret


def leaky_relu(p=0.1):
    """åˆ›å»ºLeakyReLUæ¿€æ´»å‡½æ•°"""
    return nn.LeakyReLU(p)


def l2norm(t):
    """
    L2å½’ä¸€åŒ– (æ²¿æœ€åä¸€ä¸ªç»´åº¦)
    ç”¨äºQKå½’ä¸€åŒ–ï¼Œæå‡è®­ç»ƒç¨³å®šæ€§
    """
    return F.normalize(t, dim=-1)


# ============================================================================
# Normalization Layers
# ============================================================================

class LayerNorm(nn.Module):
    """
    Bias-less LayerNorm (Layer Normalization without bias)

    Features:
    - Does not use bias parameter
    - Follows design of modern models like T5, PaLM
    - More stable training (baseline implementation)
    """

    def __init__(self, dim):
        super().__init__()
        # gamma: Learnable scaling parameter
        self.gamma = nn.Parameter(torch.ones(dim))
        # beta: Fixed bias at 0 (not learnable)
        self.register_buffer("beta", torch.zeros(dim))

    def forward(self, x):
        return F.layer_norm(x, x.shape[-1:], self.gamma, self.beta)


class RMSNorm(nn.Module):
    """
    RMSNorm (Root Mean Square Normalization)

    Faster than LayerNorm - removes mean centering step.
    Used in modern models like LLaMA, T5.

    Performance: 5-10% speedup vs LayerNorm
    """

    def __init__(self, dim, eps=1e-8):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))

    def forward(self, x):
        rms = x.pow(2).mean(-1, keepdim=True).sqrt()
        return x * (self.weight / (rms + self.eps))


# ============================================================================
# Activation Functions
# ============================================================================

class SwiGLU(nn.Module):
    """
    SwiGLU (Swish-Gated Linear Unit)

    Used in modern models like LLaMA, PaLM.
    Generally better performance than GEGLU.

    Formula: SwiGLU(x) = Swish(x_gate) * x_value
    where Swish(x) = x * sigmoid(x) = x * Ïƒ(x)
    """

    def forward(self, x):
        a, b = x.chunk(2, dim=-1)
        return F.silu(a) * b

# ============================================================================
# GEGLU Activation (é—¨æ§æ¿€æ´»å‡½æ•°)
# ============================================================================

class GEGLU(nn.Module):
    """
    GEGLU (Gated GLU with GELU activation)

    å…¬å¼: GEGLU(x) = GELU(x_gate) * x_value
    å…¶ä¸­ x_gate, x_value = x.chunk(2)

    ç‰¹ç‚¹ï¼š
    - æ¯”æ ‡å‡†ReLU/GELUæ€§èƒ½æ›´å¥½
    - ç”¨äºFeedForwardç½‘ç»œ

    ğŸ”§ [ç°ä»£åŒ–æ”¹é€ ç‚¹] å¯ä»¥å‡çº§ä¸ºï¼š
    - SwiGLU: ä½¿ç”¨Swishæ¿€æ´»å‡½æ•°ä»£æ›¿GELU
      å…¬å¼: Swish(x_gate) * x_value
      å‚è€ƒ: LLaMA, PaLMæ¨¡å‹
      æ€§èƒ½: é€šå¸¸æ¯”GEGLUç•¥å¥½
    """

    def forward(self, x):
        # å°†è¾“å…¥åˆ†æˆä¸¤åŠï¼šgateå’Œvalue
        x, gate = x.chunk(2, dim=-1)
        # ç”¨GELUæ¿€æ´»gateï¼Œç„¶åä¸valueç›¸ä¹˜
        return F.gelu(gate) * x


# ============================================================================
# FeedForward Network (å‰é¦ˆç½‘ç»œ)
# ============================================================================

def FeedForward(dim, mult=4, dropout=0., use_swiglu=False):
    """
    FeedForward Network

    Architecture:
        LayerNorm â†’ Linear(expand) â†’ Activation â†’ Dropout â†’ Linear(compress)

    Args:
        dim: Input/output dimension
        mult: Hidden layer expansion multiplier (default 4x)
        dropout: Dropout ratio
        use_swiglu: Use SwiGLU instead of GEGLU (default False for baseline)

    Inner dimension calculation:
        inner_dim = dim * mult * (2/3)
        - When mult=4, inner_dim â‰ˆ 2.67 * dim
        - Multiply by 2 because gated activations split into two halves
    """
    inner_dim = int(mult * (2 / 3) * dim)

    # Choose activation function
    activation = SwiGLU() if use_swiglu else GEGLU()

    return nn.Sequential(
        nn.LayerNorm(dim),                          # Normalization
        nn.Linear(dim, inner_dim * 2, bias=False),  # Expand (Ã—2 for gating)
        activation,                                  # Gated activation
        nn.Dropout(dropout),                        # Dropout
        nn.Linear(inner_dim, dim, bias=False)       # Compress back
    )


# ============================================================================
# PEG (Position Encoding Generator)
# ============================================================================

class PEG(nn.Module):
    """
    PEG (Position Encoding Generator) - ä½ç½®ç¼–ç ç”Ÿæˆå™¨

    ä½¿ç”¨3Dæ·±åº¦å¯åˆ†ç¦»å·ç§¯ç”Ÿæˆä½ç½®ç¼–ç 

    ç‰¹ç‚¹ï¼š
    - åŠ¨æ€ç”Ÿæˆä½ç½®ç¼–ç ï¼ˆä¸æ˜¯å›ºå®šçš„ï¼‰
    - ä½¿ç”¨groups=dimçš„å·ç§¯ï¼ˆæ¯ä¸ªé€šé“ç‹¬ç«‹ï¼‰
    - æ”¯æŒå› æœpaddingï¼ˆç”¨äºæ—¶é—´ç»´åº¦ï¼‰

    å·¥ä½œåŸç†ï¼š
    1. é€šè¿‡3x3x3çš„æ·±åº¦å·ç§¯æ•è·å±€éƒ¨ä½ç½®ä¿¡æ¯
    2. ä¸åŸå§‹ç‰¹å¾ç›¸åŠ ï¼Œä¸ºæ¯ä¸ªä½ç½®æ³¨å…¥ä½ç½®ä¿¡æ¯

    Args:
        dim: ç‰¹å¾ç»´åº¦
        causal: æ˜¯å¦ä½¿ç”¨å› æœpaddingï¼ˆæ—¶é—´ç»´åº¦åªçœ‹è¿‡å»ï¼‰

    ğŸ”§ [ç°ä»£åŒ–æ”¹é€ ç‚¹] å¯ä»¥ä¼˜åŒ–ä¸ºï¼š
    1. ä½¿ç”¨å¯åˆ†ç¦»å·ç§¯ (Depthwise-Separable Conv):
       - Conv3D(3x3x3) â†’ Conv3D(3x1x1) + Conv3D(1x3x1) + Conv3D(1x1x3)
       - å‚æ•°é‡å’Œè®¡ç®—é‡å¤§å¹…å‡å°‘

    2. å¯é€‰æ‹©ç¦ç”¨PEG:
       - å¦‚æœä½¿ç”¨RoPEç­‰å…¶ä»–ä½ç½®ç¼–ç ï¼Œå¯èƒ½ä¸éœ€è¦PEG
       - æŸäº›ä»»åŠ¡ä¸‹PEGæå‡æœ‰é™

    3. ä½¿ç”¨æ›´è½»é‡çš„MLP:
       - ç”¨å°å‹MLPä»£æ›¿å·ç§¯ç”Ÿæˆä½ç½®ç¼–ç 
    """

    def __init__(self, dim, causal=False):
        super().__init__()
        self.causal = causal
        # 3Dæ·±åº¦å¯åˆ†ç¦»å·ç§¯ (æ¯ä¸ªé€šé“ç‹¬ç«‹ï¼Œgroups=dim)
        self.dsconv = nn.Conv3d(dim, dim, 3, groups=dim)

    @beartype
    def forward(self, x, shape: Tuple[int, int, int, int] = None):
        """
        Args:
            x: è¾“å…¥ç‰¹å¾ (B, N, D) æˆ– (B, T, H, W, D)
            shape: å¦‚æœè¾“å…¥æ˜¯(B, N, D)ï¼Œéœ€è¦æä¾›åŸå§‹å½¢çŠ¶(B, T, H, W)

        Returns:
            ä½ç½®ç¼–ç åçš„ç‰¹å¾
        """
        needs_shape = x.ndim == 3
        assert not (needs_shape and not exists(shape))

        orig_shape = x.shape

        # å¦‚æœæ˜¯flattençš„ï¼Œå…ˆreshapeå›æ¥
        if needs_shape:
            x = x.reshape(*shape, -1)

        # è½¬æ¢ç»´åº¦é¡ºåº: (B, T, H, W, D) -> (B, D, T, H, W)
        x = rearrange(x, 'b ... d -> b d ...')

        # Paddingç­–ç•¥
        # ç©ºé—´ç»´åº¦(H, W): ä¸¤è¾¹å„padding 1 -> (1, 1, 1, 1)
        # æ—¶é—´ç»´åº¦(T): æ ¹æ®causalé€‰æ‹©
        #   - causal=True: åªpaddingå‰é¢ -> (2, 0) åªçœ‹è¿‡å»
        #   - causal=False: ä¸¤è¾¹å„padding 1 -> (1, 1)
        frame_padding = (2, 0) if self.causal else (1, 1)
        x = F.pad(x, (1, 1, 1, 1, *frame_padding), value=0.)

        # åº”ç”¨3Då·ç§¯
        x = self.dsconv(x)

        # è½¬å›åŸæ¥çš„ç»´åº¦é¡ºåº
        x = rearrange(x, 'b d ... -> b ... d')

        # å¦‚æœåŸæ¥æ˜¯flattençš„ï¼Œflattenå›å»
        if needs_shape:
            x = rearrange(x, 'b ... d -> b (...) d')

        return x.reshape(orig_shape)
