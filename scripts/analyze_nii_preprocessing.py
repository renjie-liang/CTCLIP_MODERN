#!/usr/bin/env python3
"""
Analyze nii.gz files to infer what preprocessing has been done.

Usage:
    python scripts/analyze_nii_preprocessing.py \
        --data-dir /path/to/valid_fixed \
        --num-samples 100
"""

import argparse
import glob
import random
from pathlib import Path
from collections import defaultdict
import numpy as np
import nibabel as nib
from tqdm import tqdm


def analyze_single_file(nii_path):
    """Analyze a single nii.gz file and extract statistics."""
    try:
        # Load NIfTI file
        nii_img = nib.load(str(nii_path))

        # Get data array
        data = nii_img.get_fdata()

        # Get header information
        header = nii_img.header
        pixdim = header['pixdim'][1:4]  # [x_spacing, y_spacing, z_spacing]

        # Extract statistics
        stats = {
            'file': Path(nii_path).name,
            'shape': data.shape,
            'dtype': str(data.dtype),
            'spacing': tuple(pixdim),
            'min_value': float(np.min(data)),
            'max_value': float(np.max(data)),
            'mean_value': float(np.mean(data)),
            'std_value': float(np.std(data)),
            'unique_values_count': len(np.unique(data)),
            'has_negative': bool(np.any(data < 0)),
            'has_values_gt_1000': bool(np.any(data > 1000)),
            'has_values_in_range_minus1_to_1': bool(np.all((data >= -1.1) & (data <= 1.1))),
        }

        # Check if values look like HU units
        # HU range: air=-1000, water=0, bone=+1000 to +3000
        stats['looks_like_HU'] = (stats['min_value'] < -500 and stats['max_value'] > 500)

        # Check if normalized to [-1, 1]
        stats['looks_normalized'] = (stats['min_value'] >= -1.05 and stats['max_value'] <= 1.05)

        return stats

    except Exception as e:
        print(f"Error analyzing {nii_path}: {e}")
        return None


def main():
    parser = argparse.ArgumentParser(description="Analyze nii.gz preprocessing status")
    parser.add_argument('--data-dir', type=str, required=True,
                        help='Directory containing nii.gz files')
    parser.add_argument('--num-samples', type=int, default=100,
                        help='Number of files to sample')
    parser.add_argument('--random-seed', type=int, default=42,
                        help='Random seed for sampling')

    args = parser.parse_args()

    data_dir = Path(args.data_dir)

    print("="*80)
    print("NIfTI Preprocessing Analysis")
    print("="*80)
    print(f"Data directory: {data_dir}")
    print(f"Sample size: {args.num_samples}")
    print()

    # Find all nii.gz files
    print("üîç Searching for nii.gz files...")
    nii_files = list(data_dir.glob("*.nii.gz"))

    if len(nii_files) == 0:
        print(f"‚ùå No nii.gz files found in {data_dir}")
        return

    print(f"   Found {len(nii_files)} total files")

    # Sample files
    random.seed(args.random_seed)
    if len(nii_files) > args.num_samples:
        sampled_files = random.sample(nii_files, args.num_samples)
        print(f"   Randomly sampling {args.num_samples} files")
    else:
        sampled_files = nii_files
        print(f"   Using all {len(nii_files)} files")

    # Analyze files
    print(f"\nüìä Analyzing {len(sampled_files)} files...")
    all_stats = []

    for nii_path in tqdm(sampled_files, desc="Analyzing", unit="file"):
        stats = analyze_single_file(nii_path)
        if stats:
            all_stats.append(stats)

    print(f"\n‚úÖ Successfully analyzed {len(all_stats)} files")

    # Aggregate statistics
    print("\n" + "="*80)
    print("AGGREGATE STATISTICS")
    print("="*80)

    # Shape distribution
    print("\nüìê Shape Distribution:")
    shape_counts = defaultdict(int)
    for s in all_stats:
        shape_counts[s['shape']] += 1

    for shape, count in sorted(shape_counts.items(), key=lambda x: -x[1]):
        percentage = (count / len(all_stats)) * 100
        print(f"   {shape}: {count} files ({percentage:.1f}%)")

    # Spacing distribution
    print("\nüìè Spacing Distribution:")
    spacing_counts = defaultdict(int)
    for s in all_stats:
        # Round to 2 decimal places for grouping
        rounded_spacing = tuple(round(x, 2) for x in s['spacing'])
        spacing_counts[rounded_spacing] += 1

    for spacing, count in sorted(spacing_counts.items(), key=lambda x: -x[1])[:10]:
        percentage = (count / len(all_stats)) * 100
        print(f"   {spacing}: {count} files ({percentage:.1f}%)")

    # Data type
    print("\nüî¢ Data Type Distribution:")
    dtype_counts = defaultdict(int)
    for s in all_stats:
        dtype_counts[s['dtype']] += 1

    for dtype, count in sorted(dtype_counts.items(), key=lambda x: -x[1]):
        percentage = (count / len(all_stats)) * 100
        print(f"   {dtype}: {count} files ({percentage:.1f}%)")

    # Value range statistics
    print("\nüìà Value Range Statistics:")
    min_values = [s['min_value'] for s in all_stats]
    max_values = [s['max_value'] for s in all_stats]
    mean_values = [s['mean_value'] for s in all_stats]
    std_values = [s['std_value'] for s in all_stats]

    print(f"   Min value range: [{np.min(min_values):.2f}, {np.max(min_values):.2f}]")
    print(f"   Max value range: [{np.min(max_values):.2f}, {np.max(max_values):.2f}]")
    print(f"   Mean value range: [{np.min(mean_values):.2f}, {np.max(mean_values):.2f}]")
    print(f"   Std value range: [{np.min(std_values):.2f}, {np.max(std_values):.2f}]")

    # Percentage with specific characteristics
    print("\nüîç Characteristic Analysis:")

    has_negative = sum(1 for s in all_stats if s['has_negative'])
    print(f"   Files with negative values: {has_negative}/{len(all_stats)} ({has_negative/len(all_stats)*100:.1f}%)")

    looks_like_HU = sum(1 for s in all_stats if s['looks_like_HU'])
    print(f"   Files that look like HU values: {looks_like_HU}/{len(all_stats)} ({looks_like_HU/len(all_stats)*100:.1f}%)")

    looks_normalized = sum(1 for s in all_stats if s['looks_normalized'])
    print(f"   Files that look normalized [-1,1]: {looks_normalized}/{len(all_stats)} ({looks_normalized/len(all_stats)*100:.1f}%)")

    has_values_gt_1000 = sum(1 for s in all_stats if s['has_values_gt_1000'])
    print(f"   Files with values > 1000: {has_values_gt_1000}/{len(all_stats)} ({has_values_gt_1000/len(all_stats)*100:.1f}%)")

    # Infer preprocessing status
    print("\n" + "="*80)
    print("PREPROCESSING INFERENCE")
    print("="*80)

    # Check if uniform spacing
    unique_spacings = len(spacing_counts)
    if unique_spacings == 1:
        spacing = list(spacing_counts.keys())[0]
        print(f"‚úÖ UNIFORM SPACING: All files have spacing {spacing}")
    else:
        print(f"‚ö†Ô∏è  VARIABLE SPACING: {unique_spacings} different spacings detected")

    # Check if uniform shape
    unique_shapes = len(shape_counts)
    if unique_shapes == 1:
        shape = list(shape_counts.keys())[0]
        print(f"‚úÖ UNIFORM SHAPE: All files have shape {shape}")
    else:
        print(f"‚ö†Ô∏è  VARIABLE SHAPE: {unique_shapes} different shapes detected")

    # Check value range
    if looks_like_HU > len(all_stats) * 0.8:
        print(f"‚úÖ HU VALUES: {looks_like_HU/len(all_stats)*100:.1f}% files appear to be in HU units")
        print(f"   ‚Üí NOT normalized yet")
    elif looks_normalized > len(all_stats) * 0.8:
        print(f"‚úÖ NORMALIZED: {looks_normalized/len(all_stats)*100:.1f}% files appear normalized to [-1, 1]")
        print(f"   ‚Üí Already normalized")
    else:
        print(f"‚ö†Ô∏è  MIXED VALUES: Unable to determine if HU or normalized")

    # Summary of preprocessing status
    print("\n" + "="*80)
    print("SUMMARY: What preprocessing has been done?")
    print("="*80)

    preprocessing_done = []
    preprocessing_todo = []

    if unique_spacings == 1:
        spacing_val = list(spacing_counts.keys())[0]
        preprocessing_done.append(f"‚úÖ Resampled to uniform spacing: {spacing_val}")
    else:
        preprocessing_todo.append("‚ùå Need to resample to uniform spacing")

    if unique_shapes == 1:
        shape_val = list(shape_counts.keys())[0]
        preprocessing_done.append(f"‚úÖ Cropped/padded to uniform shape: {shape_val}")
    else:
        preprocessing_todo.append("‚ùå Need to crop/pad to uniform shape")

    if looks_normalized > len(all_stats) * 0.8:
        preprocessing_done.append("‚úÖ Normalized to [-1, 1] range")
    else:
        preprocessing_todo.append("‚ùå Need to normalize to [-1, 1]")

    print("\nPreprocessing DONE:")
    for item in preprocessing_done:
        print(f"   {item}")

    print("\nPreprocessing TODO:")
    if preprocessing_todo:
        for item in preprocessing_todo:
            print(f"   {item}")
    else:
        print("   None - fully preprocessed!")

    # Show a few example files with detailed stats
    print("\n" + "="*80)
    print("EXAMPLE FILES (first 5)")
    print("="*80)

    for i, stats in enumerate(all_stats[:5]):
        print(f"\n[{i+1}] {stats['file']}")
        print(f"    Shape: {stats['shape']}")
        print(f"    Spacing: {stats['spacing']}")
        print(f"    Dtype: {stats['dtype']}")
        print(f"    Value range: [{stats['min_value']:.2f}, {stats['max_value']:.2f}]")
        print(f"    Mean: {stats['mean_value']:.2f}, Std: {stats['std_value']:.2f}")
        print(f"    Looks like HU: {stats['looks_like_HU']}")
        print(f"    Looks normalized: {stats['looks_normalized']}")

    print("\n" + "="*80)
    print("Analysis complete!")
    print("="*80)


if __name__ == "__main__":
    main()
