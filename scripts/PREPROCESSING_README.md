# CT-RATE æ•°æ®é¢„å¤„ç†æŒ‡å—

æœ¬æŒ‡å—ä»‹ç»å¦‚ä½•ä» Hugging Face ç›´æ¥æ„å»ºé¢„å¤„ç†åçš„ WebDatasetï¼Œä»¥å®ç° **10x è®­ç»ƒåŠ é€Ÿ**ã€‚

## ğŸ¯ ç›®æ ‡

- å°† CPU å¯†é›†å‹é¢„å¤„ç†ï¼ˆresize, normalize ç­‰ï¼‰æå‰å®Œæˆ
- è®­ç»ƒæ—¶åªéœ€å¿«é€Ÿè¯»å–é¢„å¤„ç†å¥½çš„æ•°æ®
- æ•°æ®åŠ è½½ä» ~4500ms é™è‡³ ~50-100ms
- GPU åˆ©ç”¨ç‡ä» 2.2% æå‡è‡³ 70-80%

## ğŸ“‹ å‰ç½®æ¡ä»¶

```bash
pip install huggingface-hub webdataset torch numpy
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–¹æ¡ˆä¸€ï¼šä½¿ç”¨ç¤ºä¾‹è„šæœ¬ï¼ˆæ¨èæ–°æ‰‹ï¼‰

```bash
# ç¼–è¾‘è„šæœ¬ä¸­çš„è·¯å¾„
vim scripts/build_dataset_example.sh

# è¿è¡Œ
bash scripts/build_dataset_example.sh
```

### æ–¹æ¡ˆäºŒï¼šæ‰‹åŠ¨è¿è¡Œï¼ˆæ¨èè¿›é˜¶ç”¨æˆ·ï¼‰

#### 1ï¸âƒ£ å…ˆå¤„ç†éªŒè¯é›†ï¼ˆæµ‹è¯•æµç¨‹ï¼‰

```bash
python scripts/build_preprocessed_dataset.py \
    --split valid \
    --output-dir /orange/xujie/liang.renjie/DATA/dataset/CT-RATE/dataset/valid_preprocessed_webdataset \
    --samples-per-shard 128 \
    --num-workers 8
```

**é¢„æœŸè¾“å‡º**ï¼š
```
ğŸ“‹ Listing files from ibrahimhamamci/CT-RATE (split=valid)...
   Found 7686 valid files
ğŸ“¦ Grouped 7686 files into 60 shards (128 samples/shard)
âœ… Found 0/60 existing shards
âš ï¸  Missing 60 shards
ğŸ”„ Processing 60 missing shards...
Processing shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60/60 [15:30<00:00, 15.5s/shard]
ğŸ“„ Generated manifest: .../manifest.json
   Total samples: 7686
   Total shards: 60
```

#### 2ï¸âƒ£ å¤„ç†è®­ç»ƒé›†

```bash
python scripts/build_preprocessed_dataset.py \
    --split train \
    --output-dir /orange/xujie/liang.renjie/DATA/dataset/CT-RATE/dataset/train_preprocessed_webdataset \
    --samples-per-shard 128 \
    --num-workers 16
```

**é¢„æœŸè¾“å‡º**ï¼š
```
ğŸ“‹ Listing files from ibrahimhamamci/CT-RATE (split=train)...
   Found 40279 train files
ğŸ“¦ Grouped 40279 files into 315 shards (128 samples/shard)
âœ… Found 0/315 existing shards
âš ï¸  Missing 315 shards
ğŸ”„ Processing 315 missing shards...
Processing shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 315/315 [82:15<00:00, 15.7s/shard]
```

## ğŸ“Š Manifest æ–‡ä»¶

æ¯ä¸ªæ•°æ®é›†éƒ½ä¼šç”Ÿæˆ `manifest.json`ï¼Œè®°å½•æ•°æ®é›†ä¿¡æ¯ï¼š

```json
{
  "dataset": "CT-RATE",
  "split": "train",
  "format": "webdataset",
  "preprocessed": true,
  "total_shards": 315,
  "total_samples": 40279,
  "sample_shape": [480, 480, 240],
  "sample_dtype": "float16",
  "num_classes": 18,
  "shards": [
    {
      "shard_index": 0,
      "filename": "shard-000000.tar",
      "num_samples": 128,
      "size_bytes": 14155776
    },
    ...
  ]
}
```

## ğŸ”§ é«˜çº§é€‰é¡¹

### å¢é‡å¤„ç†ï¼ˆç»­ä¼ ï¼‰

è„šæœ¬ä¼šè‡ªåŠ¨æ£€æµ‹å·²å­˜åœ¨çš„ shardsï¼Œåªå¤„ç†ç¼ºå¤±çš„éƒ¨åˆ†ï¼š

```bash
# å¦‚æœä¸­æ–­ï¼Œç›´æ¥é‡æ–°è¿è¡Œå³å¯ç»­ä¼ 
python scripts/build_preprocessed_dataset.py \
    --split train \
    --output-dir /path/to/output \
    --num-workers 16
```

### å¼ºåˆ¶é‡æ–°å¤„ç†

```bash
python scripts/build_preprocessed_dataset.py \
    --split train \
    --output-dir /path/to/output \
    --force  # é‡æ–°å¤„ç†æ‰€æœ‰ shards
```

### è‡ªå®šä¹‰ shard å¤§å°

```bash
# æ¯ä¸ª shard åŒ…å« 256 ä¸ªæ ·æœ¬ï¼ˆæ›´å¤§çš„æ–‡ä»¶ï¼Œæ›´å°‘çš„ shardsï¼‰
python scripts/build_preprocessed_dataset.py \
    --split train \
    --output-dir /path/to/output \
    --samples-per-shard 256
```

### è°ƒæ•´å¹¶è¡Œåº¦

```bash
# æ ¹æ® CPU æ ¸å¿ƒæ•°è°ƒæ•´
python scripts/build_preprocessed_dataset.py \
    --split train \
    --output-dir /path/to/output \
    --num-workers 32  # æ›´å¤šå¹¶è¡Œä¸‹è½½å’Œå¤„ç†
```

## ğŸ“ è¾“å‡ºç›®å½•ç»“æ„

```
valid_preprocessed_webdataset/
â”œâ”€â”€ manifest.json           # æ•°æ®é›†å…ƒä¿¡æ¯
â”œâ”€â”€ shard-000000.tar        # Shard 0 (128 samples)
â”œâ”€â”€ shard-000001.tar        # Shard 1 (128 samples)
â”œâ”€â”€ ...
â””â”€â”€ shard-000059.tar        # Shard 59

train_preprocessed_webdataset/
â”œâ”€â”€ manifest.json
â”œâ”€â”€ shard-000000.tar
â”œâ”€â”€ shard-000001.tar
â”œâ”€â”€ ...
â””â”€â”€ shard-000314.tar        # Shard 314 (æœ€åä¸€ä¸ªå¯èƒ½ä¸æ»¡ 128)
```

æ¯ä¸ª tar æ–‡ä»¶å†…éƒ¨ç»“æ„ï¼ˆWebDataset æ ¼å¼ï¼‰ï¼š
```
shard-000000.tar
â”œâ”€â”€ sample_001.bin          # é¢„å¤„ç†åçš„ volume (480x480x240 float16)
â”œâ”€â”€ sample_001.txt          # æŠ¥å‘Šæ–‡æœ¬
â”œâ”€â”€ sample_001.cls          # ç–¾ç—…æ ‡ç­¾ (18 classes)
â”œâ”€â”€ sample_001.json         # å…ƒæ•°æ®
â”œâ”€â”€ sample_002.bin
â”œâ”€â”€ sample_002.txt
â”œâ”€â”€ ...
```

## ğŸ”„ æ›´æ–°è®­ç»ƒé…ç½®

å¤„ç†å®Œæˆåï¼Œæ›´æ–°ä½ çš„é…ç½®æ–‡ä»¶ï¼š

```yaml
data:
  # ä½¿ç”¨é¢„å¤„ç†åçš„æ•°æ®é›†
  train_shard_pattern: "/orange/xujie/liang.renjie/DATA/dataset/CT-RATE/dataset/train_preprocessed_webdataset/shard-{000000..000314}.tar"
  valid_shard_pattern: "/orange/xujie/liang.renjie/DATA/dataset/CT-RATE/dataset/valid_preprocessed_webdataset/shard-{000000..000059}.tar"

  # å¯ç”¨å¿«é€ŸåŠ è½½æ¨¡å¼
  preprocessed: true

  # å¯ä»¥å‡å°‘ num_workersï¼ˆé¢„å¤„ç†å·²å®Œæˆï¼Œä¸éœ€è¦é‚£ä¹ˆå¤š CPUï¼‰
  num_workers: 8  # ä» 24 é™è‡³ 8
```

## âœ… éªŒè¯æ•°æ®æ­£ç¡®æ€§

å¦‚æœä½ ä¹‹å‰æœ‰ `train_fixed_webdataset` æ•°æ®ï¼Œå¯ä»¥éªŒè¯é¢„å¤„ç†æ˜¯å¦æ­£ç¡®ï¼š

```bash
python scripts/verify_preprocessed_data.py \
    --original-pattern "/path/to/train_fixed_webdataset/shard-{000000..000001}.tar" \
    --preprocessed-pattern "/path/to/train_preprocessed_webdataset/shard-{000000..000001}.tar" \
    --num-samples 10
```

**é¢„æœŸè¾“å‡º**ï¼š
```
âœ… All samples passed verification!
```

## ğŸ’¾ å­˜å‚¨ç©ºé—´ä¼°ç®—

- **åŸå§‹æ•°æ®** (npz, å˜é•¿)ï¼šçº¦ 14TB
- **é¢„å¤„ç†æ•°æ®** (å›ºå®šå¤§å°)ï¼šçº¦ 4TB
  - æ¯ä¸ªæ ·æœ¬ï¼š480 Ã— 480 Ã— 240 Ã— 2 bytes = 110 MB
  - 40,279 è®­ç»ƒæ ·æœ¬ï¼šçº¦ 4.3 TB
  - 7,686 éªŒè¯æ ·æœ¬ï¼šçº¦ 822 GB

## â±ï¸ å¤„ç†æ—¶é—´ä¼°ç®—

åŸºäº num_workers=16ï¼š

- **éªŒè¯é›†**ï¼ˆ7,686 samplesï¼‰ï¼šçº¦ 15-20 åˆ†é’Ÿ
- **è®­ç»ƒé›†**ï¼ˆ40,279 samplesï¼‰ï¼šçº¦ 80-120 åˆ†é’Ÿ

å®é™…æ—¶é—´å–å†³äºï¼š
- ç½‘ç»œé€Ÿåº¦ï¼ˆä¸‹è½½ HF æ•°æ®ï¼‰
- CPU æ ¸å¿ƒæ•°ï¼ˆå¹¶è¡Œå¤„ç†ï¼‰
- ç£ç›˜ I/O é€Ÿåº¦

## ğŸ› æ•…éšœæ’é™¤

### é—®é¢˜ 1ï¼šä¸‹è½½å¤±è´¥

```bash
âŒ Failed to download dataset/train_fixed/sample_001.npz: Connection timeout
```

**è§£å†³æ–¹æ¡ˆ**ï¼šé‡æ–°è¿è¡Œè„šæœ¬ï¼Œå®ƒä¼šè‡ªåŠ¨ç»­ä¼ ï¼Œåªå¤„ç†ç¼ºå¤±çš„ shardsã€‚

### é—®é¢˜ 2ï¼šå†…å­˜ä¸è¶³

```bash
MemoryError: Unable to allocate array
```

**è§£å†³æ–¹æ¡ˆ**ï¼šå‡å°‘ `--num-workers`ï¼š

```bash
python scripts/build_preprocessed_dataset.py \
    --split train \
    --output-dir /path/to/output \
    --num-workers 4  # é™ä½å¹¶è¡Œåº¦
```

### é—®é¢˜ 3ï¼šç£ç›˜ç©ºé—´ä¸è¶³

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. å…ˆå¤„ç†ä¸€éƒ¨åˆ†æ•°æ®
2. è„šæœ¬ä¼šè‡ªåŠ¨æ¸…ç†ä¸´æ—¶ä¸‹è½½çš„æ–‡ä»¶
3. ç¡®ä¿è‡³å°‘æœ‰ 5TB å¯ç”¨ç©ºé—´

### é—®é¢˜ 4ï¼šHuggingFace è®¤è¯

å¦‚æœæ•°æ®é›†éœ€è¦è®¤è¯ï¼š

```bash
# è®¾ç½® HF token
export HF_TOKEN="your_token_here"

# æˆ–è€…ä½¿ç”¨ huggingface-cli
huggingface-cli login
```

## ğŸ“ˆ æ€§èƒ½æå‡

ä½¿ç”¨é¢„å¤„ç†æ•°æ®åçš„é¢„æœŸæå‡ï¼š

| æŒ‡æ ‡ | ä¹‹å‰ | ä¹‹å | æå‡ |
|------|------|------|------|
| æ•°æ®åŠ è½½æ—¶é—´ | ~4500ms | ~50-100ms | **45-90x** |
| GPU åˆ©ç”¨ç‡ | 2.2% | 70-80% | **32-36x** |
| æ•´ä½“è®­ç»ƒé€Ÿåº¦ | 4.8s/step | ~0.5s/step | **~10x** |
| CPU æ ¸å¿ƒéœ€æ±‚ | 60 threads | 16 threads | **èŠ‚çœ 73%** |

## ğŸ” å·¥ä½œåŸç†

### åŸå§‹æµç¨‹ï¼ˆæ…¢ï¼‰
```
è®­ç»ƒå¾ªç¯æ¯ä¸€æ­¥ï¼š
1. ä» tar è¯»å– npz (100ms)
2. è§£å‹ npz (50ms)
3. Rescale (250ms)
4. Clip (127ms)
5. Resize (262ms)
6. Normalize (135ms)
7. Crop/Pad (50ms)
8. GPU æ“ä½œ (379ms)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
æ€»è®¡ï¼š~1350ms/step
```

### é¢„å¤„ç†æµç¨‹ï¼ˆå¿«ï¼‰
```
ä¸€æ¬¡æ€§é¢„å¤„ç†ï¼š
1-7. æ‰€æœ‰é¢„å¤„ç†æ“ä½œ â†’ ä¿å­˜ä¸º WebDataset

è®­ç»ƒå¾ªç¯æ¯ä¸€æ­¥ï¼š
1. ä» tar è¯»å–å·²å¤„ç†æ•°æ® (30ms)
2. Permute + Unsqueeze (0.02ms)
3. GPU æ“ä½œ (379ms)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
æ€»è®¡ï¼š~410ms/step
```

## ğŸ“š ç›¸å…³è„šæœ¬

- `build_preprocessed_dataset.py` - ä¸»è„šæœ¬ï¼ˆä» HF æ„å»ºé¢„å¤„ç†æ•°æ®é›†ï¼‰
- `preprocess_webdataset.py` - è½¬æ¢å·²æœ‰çš„ WebDataset
- `verify_preprocessed_data.py` - éªŒè¯é¢„å¤„ç†æ­£ç¡®æ€§
- `inspect_webdataset.py` - æ£€æŸ¥ WebDataset å†…å®¹

## ğŸ’¡ æç¤º

1. **å…ˆæµ‹è¯•å°æ•°æ®é›†**ï¼šå…ˆå¤„ç†éªŒè¯é›†ï¼ˆæ›´å°ï¼‰ï¼Œç¡®è®¤æµç¨‹æ­£ç¡®
2. **ä½¿ç”¨ tmux/screen**ï¼šå¤„ç†è®­ç»ƒé›†éœ€è¦ 1-2 å°æ—¶ï¼Œä½¿ç”¨æŒä¹…ä¼šè¯
3. **ç›‘æ§è¿›åº¦**ï¼šè„šæœ¬ä¼šæ˜¾ç¤ºè¿›åº¦æ¡å’ŒæˆåŠŸ/å¤±è´¥ç»Ÿè®¡
4. **ä¿ç•™ manifest**ï¼š`manifest.json` åŒ…å«é‡è¦çš„æ•°æ®é›†ä¿¡æ¯
5. **å¢é‡å¤„ç†**ï¼šä¸­æ–­åé‡æ–°è¿è¡Œä¼šè‡ªåŠ¨ç»­ä¼ 

## ğŸ“ è·å–å¸®åŠ©

```bash
python scripts/build_preprocessed_dataset.py --help
```
