# Multi-GPU Training Guide

## ğŸ“Š é‡è¦ï¼šIterationæ•°é‡å˜åŒ–

ä½¿ç”¨å¤šGPUä¼šå‡å°‘æ¯ä¸ªepochçš„iterationsï¼Œä½†**æ€»æ•°æ®é‡ä¿æŒä¸å˜**ï¼š

| é…ç½® | Batch Size (per GPU) | Total Batch | Steps/Epoch |
|------|---------------------|-------------|-------------|
| 1 GPU | 4 | 4 | 7,375 |
| 2 GPUs | 4 | 8 | 3,687 (50%) |
| 4 GPUs | 4 | 16 | 1,843 (25%) |
| 8 GPUs | 4 | 32 | 921 (12.5%) |

**è¯´æ˜ï¼š**
- Stepså‡å°‘æ˜¯å› ä¸ºæ¯æ­¥å¤„ç†æ›´å¤šæ•°æ®
- æ¯ä¸ªepochä»ç„¶éå†æ‰€æœ‰29,500ä¸ªæ ·æœ¬
- è®­ç»ƒæ—¶é—´ä¼šå‡å°‘ï¼ˆå¹¶è¡ŒåŠ é€Ÿï¼‰

---

## æ–¹æ¡ˆ1ï¸âƒ£: å•èŠ‚ç‚¹å¤šGPUï¼ˆæœ€ç®€å•ï¼‰

### ä½¿ç”¨åœºæ™¯
- ä¸€å°æœºå™¨ä¸Šæœ‰å¤šä¸ªGPU
- æœ€ç®€å•çš„å¹¶è¡Œæ–¹æ¡ˆ
- æ¨èå…ˆæµ‹è¯•è¿™ä¸ª

### æ­¥éª¤

**1. ä¿®æ”¹é…ç½®æ–‡ä»¶ `accelerate_config_single_node.yaml`**

```yaml
num_processes: 2  # æ”¹æˆä½ æƒ³ç”¨çš„GPUæ•°é‡ (2, 4, 8ç­‰)
```

**2. å¯åŠ¨è®­ç»ƒ**

```bash
# æ–¹å¼A: ä½¿ç”¨è„šæœ¬
bash scripts/train_multi_gpu.sh

# æ–¹å¼B: ç›´æ¥å‘½ä»¤
accelerate launch \
    --config_file accelerate_config_single_node.yaml \
    train.py \
    --config configs/base_config.yaml
```

**3. æŒ‡å®šç‰¹å®šGPUï¼ˆå¯é€‰ï¼‰**

```bash
# åªä½¿ç”¨GPU 0å’Œ1
export CUDA_VISIBLE_DEVICES=0,1
bash scripts/train_multi_gpu.sh

# åªä½¿ç”¨GPU 2å’Œ3
export CUDA_VISIBLE_DEVICES=2,3
bash scripts/train_multi_gpu.sh
```

---

## æ–¹æ¡ˆ2ï¸âƒ£: å¤šèŠ‚ç‚¹å¤šGPUï¼ˆSLURMï¼‰

### ä½¿ç”¨åœºæ™¯
- éœ€è¦ä½¿ç”¨å¤šå°æœºå™¨
- æœ‰SLURMä½œä¸šè°ƒåº¦å™¨
- éœ€è¦æ›´å¤§è§„æ¨¡è®­ç»ƒ

### æ­¥éª¤

**1. ä¿®æ”¹SLURMè„šæœ¬ `scripts/train_slurm_multi_node.sh`**

æ ¹æ®ä½ çš„é›†ç¾¤ä¿®æ”¹ï¼š
```bash
#SBATCH --nodes=2                   # èŠ‚ç‚¹æ•°é‡
#SBATCH --gpus-per-node=4          # æ¯èŠ‚ç‚¹GPUæ•°é‡
#SBATCH --partition=gpu            # åˆ†åŒºåç§°
```

**2. ä¿®æ”¹é…ç½®æ–‡ä»¶ `accelerate_config_multi_node.yaml`**

```yaml
num_machines: 2      # èŠ‚ç‚¹æ•°é‡
num_processes: 8     # æ€»GPUæ•° = nodes Ã— GPUs per node
```

**3. æäº¤ä½œä¸š**

```bash
sbatch scripts/train_slurm_multi_node.sh
```

**4. æŸ¥çœ‹æ—¥å¿—**

```bash
# æŸ¥çœ‹è¾“å‡º
tail -f logs/train_JOBID.out

# æŸ¥çœ‹é”™è¯¯
tail -f logs/train_JOBID.err
```

---

## âš™ï¸ éœ€è¦è°ƒæ•´Learning Rateå—ï¼Ÿ

å½“batch sizeå¢å¤§æ—¶ï¼Œé€šå¸¸éœ€è¦è°ƒæ•´learning rateï¼š

### Linear Scaling Rule
```
æ–°LR = åŸLR Ã— (æ–°batch / åŸbatch)
```

**ç¤ºä¾‹ï¼š**
```yaml
# åŸé…ç½® (1 GPU, batch=4)
learning_rate: 1.25e-6

# 2 GPUs (total batch=8)
learning_rate: 2.5e-6  # 1.25e-6 Ã— 2

# 4 GPUs (total batch=16)
learning_rate: 5.0e-6  # 1.25e-6 Ã— 4
```

**ä½†è¦æ³¨æ„ï¼š**
- å¯¹äºå°batch size (< 256)ï¼Œå¯èƒ½ä¸éœ€è¦çº¿æ€§ç¼©æ”¾
- å»ºè®®å…ˆæµ‹è¯•åŸLRï¼Œå¦‚æœä¸ç¨³å®šå†è°ƒæ•´
- å¯ä»¥é…åˆæ›´é•¿çš„warmup

---

## ğŸ” éªŒè¯å¤šGPUæ˜¯å¦ç”Ÿæ•ˆ

è®­ç»ƒå¼€å§‹æ—¶ä¼šæ˜¾ç¤ºï¼š

```
Distributed environment: MULTI_GPU
Number of processes: 2
Number of GPUs: 2
```

ä½¿ç”¨ `nvidia-smi` æŸ¥çœ‹GPUä½¿ç”¨ï¼š
```bash
watch -n 1 nvidia-smi
```

åº”è¯¥çœ‹åˆ°å¤šä¸ªGPUéƒ½æœ‰æ˜¾å­˜å ç”¨å’ŒGPUåˆ©ç”¨ç‡ã€‚

---

## ğŸ› å¸¸è§é—®é¢˜

### 1. æŠ¥é”™ï¼šNCCL timeout
**åŸå› ï¼š** èŠ‚ç‚¹é—´ç½‘ç»œé€šä¿¡é—®é¢˜

**è§£å†³ï¼š**
```python
# åœ¨ trainer.py ä¸­å·²è®¾ç½®è¶…æ—¶
init_kwargs = InitProcessGroupKwargs(timeout=timedelta(seconds=36000))
```

### 2. æŠ¥é”™ï¼šOut of memory
**åŸå› ï¼š** æ¯ä¸ªGPUä»ç„¶åŠ è½½ç›¸åŒçš„batch_size

**è§£å†³ï¼š** å‡å°é…ç½®ä¸­çš„batch_size
```yaml
data:
  batch_size: 2  # ä»4å‡åˆ°2
```

### 3. Losséœ‡è¡
**åŸå› ï¼š** Batch sizeå˜å¤§å¯¼è‡´è®­ç»ƒä¸ç¨³å®š

**è§£å†³ï¼š**
- å¢å¤§warmup_steps
- é™ä½learning rate
- ä½¿ç”¨gradient accumulation

---

## ğŸ“ˆ æ€§èƒ½å¯¹æ¯”

é¢„æœŸåŠ é€Ÿæ¯”ï¼ˆç†æƒ³æƒ…å†µï¼‰ï¼š

| GPUs | ç†è®ºåŠ é€Ÿ | å®é™…åŠ é€Ÿ | é€šä¿¡å¼€é”€ |
|------|----------|----------|----------|
| 1 | 1.0x | 1.0x | 0% |
| 2 | 2.0x | 1.8-1.9x | 5-10% |
| 4 | 4.0x | 3.5-3.8x | 5-12% |
| 8 | 8.0x | 6.5-7.0x | 12-18% |

å•èŠ‚ç‚¹é€šå¸¸æ¯”å¤šèŠ‚ç‚¹æ•ˆç‡æ›´é«˜ï¼ˆé€šä¿¡å»¶è¿Ÿæ›´ä½ï¼‰ã€‚

---

## ğŸ¯ å»ºè®®çš„è®­ç»ƒæµç¨‹

**ç¬¬1æ­¥ï¼šå•GPUéªŒè¯ä»£ç ** âœ… (ä½ å·²å®Œæˆ)
```bash
python train.py --config configs/debug_config.yaml
```

**ç¬¬2æ­¥ï¼šå•èŠ‚ç‚¹2 GPUæµ‹è¯•**
```bash
# ä¿®æ”¹ accelerate_config_single_node.yaml: num_processes: 2
bash scripts/train_multi_gpu.sh
```

**ç¬¬3æ­¥ï¼šå•èŠ‚ç‚¹å…¨éƒ¨GPU**
```bash
# ä¿®æ”¹ accelerate_config_single_node.yaml: num_processes: 4 (æˆ–ä½ çš„GPUæ•°)
bash scripts/train_multi_gpu.sh
```

**ç¬¬4æ­¥ï¼šï¼ˆå¯é€‰ï¼‰å¤šèŠ‚ç‚¹è®­ç»ƒ**
```bash
# ä¿®æ”¹ SLURM è„šæœ¬å’Œé…ç½®
sbatch scripts/train_slurm_multi_node.sh
```

---

## ğŸ“ éœ€è¦å¸®åŠ©ï¼Ÿ

å¦‚æœé‡åˆ°é—®é¢˜ï¼Œæ£€æŸ¥ï¼š
1. `nvidia-smi` - ç¡®è®¤GPUå¯è§
2. æ—¥å¿—ä¸­çš„ "Number of processes" - ç¡®è®¤GPUæ•°é‡æ­£ç¡®
3. Steps per epoch - åº”è¯¥å‡å°‘åˆ°åŸæ¥çš„ 1/N (N=GPUæ•°é‡)
